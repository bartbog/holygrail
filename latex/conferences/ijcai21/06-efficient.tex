% \todo{make naming consistent: OCUS instead of OCUS}
While our OCUS algorithm in~\ref{alg:comus} is generic and can also be used to find OUSs, its constrainedness property is key to decreasing the complexity of explanation sequence generation.
\emilio{Note that, OUS are found by removing predicate $p$. In algorithm \ref{alg:comus}, this boils down to replacing line 2 with $\cohs(\setstohit,f,True)$ }
 We now discuss the optimizations to the OCUS algorithm that are specific to explanation sequence generation, though they can also be used when other forms of domain knowledge are present. 
%In this section, we discuss optimizations to the basic algorithm to compute OCUSs. 
%While our optimizations are inspired by the explanation generation problem, and tailored to solving this as efficiently as possible, the ideas presented here can also 
 

\paragraph{Incremental OCUS computation}
%While searching for unsatisfiable subsets satisfying structural constraints already greatly reduces the number of calls to the unsatisfiable subset oracle, i
Inherently, generating a sequence of explanations still requires as many OCUS calls as there are literals to explain. 
Indeed, a greedy sequence construction algorithm %high-level algorithm, not explicitly portrayed in the current paper, 
calls \onestepo iteratively with a growing interpretation $I$ until $I=\Iend$.

All of these calls to \onestepo, and hence OCUS, are done with very similar input (the set of constraints does not change, and the $I$ only marginally changes between two calls). For this reason, it makes sense that information computed during one of the earlier stages can be useful in later stages as well. 

The main question is, suppose two \comus calls are done, first with inputs $\formula_1$, $f_1$, and $p_1$, and later with $\formula_2$, $f_2$, and $p_2$; how can we make use as much as possible of the data computations of the first call to speed-up the second call? The answer is surprisingly elegant. The most important data \comus keeps track of  is the collection \setstohit of sets-to-hit.

This collection in itself is not useful for transfer between two calls, since -- unless we assume that $\formula_2$ is a subset of $\formula_1$, there is no reason to assume that a set in $\setstohit_1$ should also be hit in the second call. 
However, each set $H$ in $\setstohit$ is the complement (with respect to the formula at hand) of a \emph{satisfiable subset} of constraints, and this satisfiability remains true. 
Thus, instead of storing $\setstohit$, we can, keep track of a set \satsets of \emph{satisfiable subsets} (the sets $\m{S}$ in the \comus algorithm). 
When a second call to \comus is performed, we can then initialize $\setstohit$ as the complement of each of these satisfiable subsets with respect to $\formula_2$, i.e., \[\setstohit\gets \{\formula_2\setminus \m{S}\mid \m{S}\in \satsets\}.\]

Our actual implementation goes a bit further: we know that all calls to $\comus(\formula,f,p)$ will be cast with $\formula \subseteq \m{C}\cup \Iend \cup \overline{\Iend \setminus I_0}$, where $I_0$ is the start interpretation. Since our implementation uses a MIP solver for computing hitting sets, and we have this upper bound on the set of formulas to be used, we can initialise all relevant decision variables once, and reuse the same MIP solving instance. We can temporarily disable every literal not in \formula by settings its weight to infinity in the objective function.
% \emilio{need to re-read reviews, reviewers asked (?) for simple MIP model of the problem}

% \paragraph{Using Initialization to Add Domain Knowledge} ? 
% \todo{TO BE DETERMIEND IF WE WANT TO SAY SOMETHING AOBUT PRESEEDING.}\bart{ I still dont believe it makes a difference. TO be tested first!} {\color{red}The current experiments confirm (finally) that this is useless. So let us just drop it.  Honestly, this gives me more trust in the experiments :-)   }



\paragraph{Domain-Specific Implementations of \grow} \label{para:domainspecificgrow}
%In the context of explanations we are in, we have a way of generating large satisfiable subsets quite easily and hence we conjecture that implementing a domain-specific \grow will be more efficient than generic version of the \grow algorithm. 

The goal of the \grow procedure is to turn $\m{S}$ into a larger subformula of $\formula$. The effect of this is that the complement added to \setstohit will be smaller, and hence, a stronger restriction for the hitting set solver is found.  

Choosing an effective \grow procedure requires finding a difficult balance: on the one hand, we want our subformula to be as large as possible, %to derive the most precise information we can get, 
but on the other hand we also want the procedure to be very efficient. 
Indeed, if the computation cost of the \grow were not important, we could use a MaxSAT call to obtain a subset-maximal subset of $\formula$, however, by doing that we expect the \grow costs to be too high to make the overall OCUS algorithm faster. 

However, for the case of explanations we are in, we make the following observations: 
\begin{compactitem}
 \item Our formula at hand (using the notation from the \onestepo algorithm) consists of three types of clauses: 1) (translations of) the problem constraints (this is \formulac) 2) literals representing the assignment found thus far (this is $I$) and 3) the negations of literals not-yet-derived (this is $\overline{\Iend\setminus I}$). 
 \item $\formulac$ and $I$ together are satisfiable and \emph{mutually supportive}, by this we mean that making more constraints in \formulac true, more literals in $I$ will automatically become true and vice versa. 
 \item The constraint $p$ enforces that each hitting set will contain \textbf{exactly} one literal of  $\overline{\Iend\setminus I}$
\end{compactitem}
Since the restriction on the third type of elements of $\formula$ are already strong, it makes sense to use the \grow procedure to search for a \emph{maximal} satisfiable subset of $\formulac\cup I$ with hard constraints that $\m{S}$ should be satisfied, using a call to an efficient  (partial) MaxSAT solver. Furthermore, we can initialize this call as well as any call to a SAT solver with the polarities for all variables set to the value they take in $\Iend$. This is likely to lead to a fairly large satisfiable subset and hence a good correction set. 

% \todo{Bart from here}


% \todo{``old maxsat'' comes here}



