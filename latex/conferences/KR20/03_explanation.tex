% !TeX root = ./workshop_paper.tex
Ideally, we could generate all explanations of each fact in $max(I_0,\allconstraints)$, and search for the lowest scoring sequence among those explanations.
However, the number of explanations for each fact quickly explodes with the number of constraints, and is hence not feasible to compute.
Instead, we will iteratively construct the sequence, by generating candidates for a given partial interpretation and searching for the smallest one among those.
Algorithm \ref{alg:nested-main} formalizes the greedy construction of the sequence, which determines $I_{end} = max(I_0,\allconstraints)$  through propagation and relies on a \textit{min-explanation$(I,\allconstraints)$} function to find the next cost-minimal explanation.

\begin{algorithm}[ht]
    $I_{end} \gets$ propagate$(I_0\land \allconstraints)$\;
    $Seq \gets$ empty sequence\;
    $I \gets I_0$\;
    \While{$I \neq I_{end}$}{
    $(E, S, N) \gets $min-explanation$(I, \allconstraints)$\;
    {\color{gray}
    $nested \gets$ nested-explanations$(E, S, N)$\;
      append $((E, S, N),nested)$ to $Seq$\;
    }
    $I \gets I \cup N$\;
    }
    \caption{greedy-explain$(I_0,$ $\allconstraints)$}
    \label{alg:nested-main}
  \end{algorithm}
% Algorithm \ref{alg:nested-main} starts from an initial interpretation $I_0$ 

At line 5 of the algorithm, \textit{min-explanation} uses propagation to get the set of new facts that can be derived from a given partial interpretation $I$ and the constraints T. 

For each new fact $n$ not in $I$, we wish to find a non-redundant explanation $(E \subseteq I, S \subseteq \allconstraints,\{n\})$ that explains $n$ (and possibly explains more).
This means, that whenever one of the facts in $E$ or constraints in $\allconstraints$ is removed, the result is no longer an explanation. This task is equivalent to finding a Minimal Unsat Core (or Minimal Unsat Subset, MUS). To see this, consider the theory
\[ I\wedge \allconstraints \wedge \lnot n. \xLleftRrightarrow{} \lnot (I \wedge T \Rightarrow n) \]
This theory surely is unsatisfiable since $n$ is a consequence of $I$ and $\allconstraints$. We see that each unsatisifiable subset of this theory is of the form $E \wedge S \wedge \lnot n$ where $(E,S,\{n\})$ is a (not necessarily redundant) explanation of the derivation of $\{n\}$ from $I$.

We must point out that MUS algorithms typically find \textit{an} unsatisfiable core that is \textit{subset-minimal}, but not \textit{cardinality-minimal}. That is, the unsat core can not be reduced further, but there could be another minimal unsat core whose size is smaller.
That means that if size is taken as a measure of simplicity of explanations, we do not have the guarantee to find the optimal ones. 
And definitely, when a cost function kicks, optimality is also not guaranteed.

In practice, to guide the search to cost-minimal MUS's, we use the observation that typically a small (1 to a few) number of constraints is sufficient to explain the reasoning. A small number of constraints is also preferred in terms of easy to understand explanations, and hence have a lower cost. For this reason, we will don't use the full set of constraints \allconstraints, but we will iteratively grow the number of constraints used.

We make one further assumption to ensure that we do not have to search for candidates for all possible subsets of constraints. The assumption is that we have an optimistic estimate $g$ that maps a subset $S$ of $\allconstraints$ to a real number such that  $\forall E, N, S: g(S) \leq f(E, S, N)$. This is for example the case if $f$ is an additive function, such as $f(E, S, N) = f_1(E) + f_2(S) + f_3(N)$ where $g(S) = f_2(S)$ assuming $f_1$ and $f_3$ are always positive.
