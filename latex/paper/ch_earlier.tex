\section{Related work}
In this section we discuss some related work. We begin with three CNL's, two for knowledge representation and one with types. Finally, we discuss another framework that is based on $\lambda$-calculus.

Two of the most advanced controlled natural languages that can be used for knowledge representation are Attempto Controlled English (ACE) and Processable English (PENG). ACE \cite{Fuchs2008} is a general purpose CNL. The language contains a large built-in vocabularium. This has as advantage that the user doesn't have to provide the vocabularium. However, for specifications in small domains, we usually do want to provide the vocabularium to make sure the specification only talks about the modelled domain. PENG \cite{Schwitter2002} does ask the user to provide its own content words. Moreover, there is a tool named ECOLE which gives suggestions while writing PENG sentences, namely about which linguistic constructs can follow the words already entered. This makes it easier to write correct PENG sentences. These languages mainly show that we can translate English into logic, but the literature on both these CNL's lacks a description of this translation process. Therefore, extending these CNL's is hard.

Another important CNL is RuleCNL \cite{Njonko2014}. It is a CNL that translates (a subset of) English into business rules. It's notable because it is the only CNL we could find that uses types. Interestingly, the paper lacks a description of the type system that was used and of the inferences that are supported. However figure~6 from \cite{Njonko2014} indicates that type checking of business rules in RuleCNL is supported.

Finally, Baral et al. \cite{Baral2008, Baral2012, Baral2012a} researched translating natural language into ASP programs. They do this based on $\lambda$-calculus and a Combinatorial Categorial Grammar (CCG). Each word gets one $\lambda$-ASP-expression per CCG category it can represent. The CCG then describes how to combine these expressions via $\lambda$-application. In their latest paper on this subject \cite{Baral2012a}, Baral et al. explain how to learn both these $\lambda$-ASP-expressions and a probabilistic CCG grammar from a set of logic grid puzzles. They used a supervised machine learning method for this goal. Based on these methods they can solve unseen puzzles (without adapting the clues of the puzzles). They do not support other kind of inferences.

% However, the goal of this paper is not to solve (unseen) logic grid puzzles without adapting them. The goal is to create a CNL with formal semantics that can be applied to unseen puzzles and can be used as a knowledge representation language (for these puzzles). This paper illustrates that for a small domain, it is possible to have such a language. For unseen puzzles, slight adaptions are necessary to make sure the clues are grammatically correct according to the constructed CNL. But as mentioned earlier, once the clues are expressed in a formal knowledge representation language, multiple inferences become possible. Solving the logic grid puzzle is only one of the examples.
