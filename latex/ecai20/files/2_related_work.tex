This research fits within the general topic of Explainable Agency~\cite{langley2017explainable}, whereby in order for people to trust autonomous agents, the latter must be able to explain their decisions and the reasoning that produced their choices. 
For example, explainable planning~\cite{fox2017explainable} is concerned with building planning systems that can explain their own behaviour. This includes answering queries such as ``why did the system (not) make a certain decision'', ``why is this the best decision'', etc. In contrast to explainable machine learning research, in explainable planning one can make use of the explicit \textit{model-based representation} over which the reasoning happens. Likewise, we will make use of the constraint specification available to constraint solvers. %, and focus on providing explanations of propagation steps in an interpretable way.

Explanation of constraint satisfaction problems has been studied mostly in the context of overconstrained problems. 
The goal then is to find which constraints conflict with each other. The QuickXplain method \cite{junker2001quickxplain} for example uses a dichotomic approach that recursively partitions the constraints to find a minimal conflict set. This is also known as the Minimum Unsat Subset (MUS) problem or Minimal Unsat Core extraction~\cite{marques2010minimal}. Many algorithm exist for finding a MUS or enumerating all MUS's~\cite{marques2010minimal}. We will use MUS extraction for finding a minimal explanation of an individual inference step.

Our work is inspired by the holy grail challenge at CP'2019, which in turn has its roots in earlier work of E.~Freuder on inference-based explanations \cite{sqalli1996inference}. In that work, the authors investigate logic grid puzzles and develop a number of problem-specific inference rules that allow solving such puzzles without search. These inference rules are equipped with explanation templates such that each propagation event of an inference rule also has a templated explanation, and hence an explanation of the solution process is obtained. We point out that the more complex inference rules (NCC and GNCC) are in inference rules over hard-coded combinations of (in)equality constraints. In contrast, our proposed method works for any type of constraint and any combination of constraints, and automatically infers a minimal set of facts and constraints that explain an inference step, without using any problem-specific knowledge. %\bart{can they explain the pasta puzzle? Probably not!} I would expect not: that combination not hard-coded

%DUPLICATE Our work is inspired by the holy grail challenge at CP'2019, which in turn has its roots in earlier work of E. Freuder on inference-based explanations \cite{sqalli1996inference}. In that work, the authors look at logic grid puzzles and develop a number of problem-specific inference rules that allow solving such puzzles without search. These inference rules are equiped with explanation templates such that each propagation event of an inference rule also has a templated explanation, and hence an explanation of the solution process is obtained. We remark that the more complex inference rules (NCC and GNCC) are over hard-coded combinations of (in)equality constraints. In contrast, our proposed method works for any type of constraint and any combination of constraints, and automatically infers a minimal set of facts and constraints that explain a small inference step, without using any problem-specific knowledge.
%Explanation of constraint satisfaction problems has been studied mostly in the context of overconstrained problems. The goal is to find what constraints conflict with each other. 
%The QuickXplain method \cite{junker2001quickxplain} for example uses a dichotomic approach that recursively partitions the constraints to find a minimal conflict set. 
%This is also known as the Minimum Unsat Subset (MUS) problem or Minimal Unsat Core extraction~\cite{marques2010minimal}. Many algorithm exist for finding a MUS or enumerating all MUS's~\cite{marques2010minimal}. We will use MUS extraction for finding a minimal explanation of an individual inference step.


There is a rich literature on automated and interactive theorem proving, recently focussing on providing proofs that are understandable for humans \cite{Ganesalingam2017} and, e.g.,  on teaching humans -- using interaction with theorem provers -- how to craft mathematical proofs  \cite{DBLP:conf/icml/YangD19}. 
Our work fits into this line of research since our generated explanations can also be seen as proofs, but in the setting of finite-domain constraint solving.

% Our work also fits within the general topic of Explainable Agency~\cite{langley2017explainable}, whereby in order for people to trust autonomous agents, the latter must be able to explain their decisions and the reasoning that produced their choices. 
% For example, explainable planning~\cite{fox2017explainable} is concerned with building planning systems that can explain their own behaviour. This includes answering queries such as ``why did the system (not) make a decision'', ``why is this the best decision'' etc. In contrast to explainable machine learning research, in explainable planning one can make use of the explicit \textit{model-based representation} over which the reasoning happens. Likewise, we will make use of the constraint specification available to constraint solvers, and focus on providing explanations of propagation steps in an interpretable way.


\todo{CHECK THIS OUT (reviewer comment):
At a high-level, there seems to be connection to other approaches to explanation. For instance, in the context of decision-aiding, this sort of step-wise explanations have been advocated, with similar concerns it seems regarding the definition of generic complexity functions:
- Belahcene et al. Explaining robust additive utility models by sequences of preference swaps. Theory and Decision, 2017
}
