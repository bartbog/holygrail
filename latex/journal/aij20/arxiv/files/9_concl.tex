In this paper, we formally defined the problem of step-wise explanation generation for satisfaction problems, as well as presenting a generic algorithm for solving the problem. We extended the mechanism so that it can be used in a \textit{nested} way, to further explain an explanation.
We developed algorithm in the context of logic grid puzzles where we start from natural language clues and provide a human-friendly explanation in the form of a visualisation. 

When investigating the nested explanation in Figure \ref{fig:zebrascreen:path} as it is produced by the system, one can observe that this explanation does not entirely match an explanation that would be produced by a human reasoner for a couple of reasons: 
\begin{itemize}
 \item In the generation of the nested explanation, as well as in the high-level sequence, we used the greedy algorithm from section \ref{sec:expl-gen-prod}. 
 While at the high level, this yields good results, at the nested level, this results in sometimes propagating facts that are not used afterwards. 
 The very first propagation in the nested explanation is of this kind.
 While this would be easy to fix by postprocessing the generated explanation, we left it in our example to highlight this difference between the nested and non-nested explanation. 
 \item It sometimes happens that the system finds, as a minimal explanation, one in which $X-1$ negative facts are used instead of the corresponding single positive fact. This can be seen in the last step. For human reasoners the positive facts often seem to be easier to grasp. A preference for the system towards these negative facts might be incidentally due to formulation of the clues or it can incidentally happen due to the way the MUS is computed (only subset-minimality is guaranteed there). 
 In general, observations of this kind should be taken into account when devising a cost function. 
 \item A last observation we made (but that is not visible in the current figure) is that sometimes the generated nested explanations seem to be unnecessarily hard. In all cases we encountered where that was the case, the explanation was the same: the set of implicit constraints contains a lot of redundant information: a small number of them would be sufficient to imply all the others. Our cost function, and the subset-minimality of the generated MUS entails that in the explanation of a single step, implicit constraints will never be included if they follow from other included implicit constraints. However, when generating the nested explanations, it would actually be preferred to have those redundant constraints, since they allow breaking up the explanation in simpler parts, e.g., giving a simple step with a single bijectivity, rather than a complex step that uses a combination of multiple implicit constraints.
\end{itemize}
These observations suggest that further research into the question \emph{what constitutes an understandable explanation for humans} is needed. Additional directions to produce easier-to-understand explanations would be optimizing the entire sequence instead of step by step, and learning the cost function based on user traces.

With respect to \emph{efficiency}, the main bottleneck of the current algorithm is the many calls to MUS, which is a hard problem by itself. 
Therefore, in future work we want to investigate unsat-core \emph{optimization} with respect to a cost-function, either by taking inspiration for instance from the MARCO algorithm~\cite{liffiton2013enumerating} but adapting it to prune based on cost-functions instead of subset-minimality, or alternatively by reduction to quantified Boolean formulas or by using techniques often used there~\cite{QBF,DBLP:journals/constraints/IgnatievJM16}.

From a systems point of view, a direction for future work is to make our approach interactive, essentially allowing \ourtool to be called \emph{while} a user is solving the puzzle and to implement in more serious domains such as for instance interactive configuration in which a human and a search engine cooperate to solve some configuration problem and the human can often be interested in understanding \emph{why} the system did certain derivations \cite{DBLP:journals/tplp/HertumDJD17,DBLP:conf/bnaic/CarbonnelleADVD19}. 

Finally, we wish to extend the approach so that it is also applicable to \textit{optimisation problems}. An additional challenge is explaining \textit{search} choices on top of propagation steps as we do in this work. Many open questions remain, but key concepts such as a sequence of simple steps and searching for simple explanations based on explanations of individual facts offer interesting starting points.
