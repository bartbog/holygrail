\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{fullpage}
\usepackage[parfill]{parskip}
\usepackage[normalem]{ulem}

\let\oldquote\quote
\let\oldendquote\endquote

\def\quoteit{\begingroup \oldquote \itshape}
\def\endquoteit{\upshape \oldendquote \endgroup}

%% Comments
\newcommand\comment[1]{\marginpar{\tiny #1}}
\renewcommand\comment[1]{#1}

\newcommand{\tias}[1]{{\comment{\color{blue}\textsc{TG:}#1}}}
\newcommand{\emilio}[1]{{\comment{\color{red} \textsc{EG:}#1}}}
\newcommand{\Revision}[1]{{\comment{Revision: #1}}}
\newcommand{\answer}[1]{{\comment{\textbf{Answer:} #1}}}
\newcommand{\tempanswer}[1]{{\comment{\color{orange} \textbf{Temporary answer:} #1}}}
\newcommand{\bart}[1]{{\comment{\color{green} \textsc{BB:}#1}}}
\newcommand{\new}[1]{{\comment{{\color{blue} #1}}}}
\title{Revisions: A framework for step-wise explaining how to solve constraint satisfaction problems}

\author{Bart Bogaerts, Emilio Gamba and Tias Guns}
\date{}
\begin{document}
	\maketitle
We thank the reviewers for their effort and constructive feedback. In the following, we answer the remarks of the reviewers and point out the changes we incorporated in this revision.

\section*{Reviewer \#1 - DETAILED COMMENTS}

\begin{quoteit}
	This paper proposes a solution not only to solve a combinatorial
	problem, but to explain to a human how to solve it. Therefore, the
	focus is not to solve the problem as fast as possible but rather to
	provide reasoning as simple as possible that leads to a solution. While
	the method seems sufficiently general to work with any problem in NP,
	the paper directs its efforts toward explaining how to solve grid
	puzzles.
The paper is well written and easy to understand. I noted below a few typos that can quickly be fixed.
\end{quoteit}

\answer{We have addressed the typos in the revised version.}

\begin{quoteit}
The literature review could be augmented with the following paper that was published before people talk about explainable AI. But in some sense, it does XAI on a grid puzzle.

Caine A., Cohen R. (2006) MITS: A Mixed-Initiative Intelligent Tutoring System for Sudoku. In: Lamontagne L., Marchand M. (eds) Advances in Artificial Intelligence. Canadian AI 2006. Lecture Notes in Computer Science, vol 4013. Springer, Berlin, Heidelberg. \url{https://doi.org/10.1007/11766247_47}
\end{quoteit}

\answer{The proposed paper has been added to the related works section: ``In the context of interactive explanations, Caine and Cohen~\cite{caine2006mits} introduce a \emph{mixed-initiative} Intelligent Tutoring System (MITS). 
	In that framework, the system maintains a model of the knowledge of the user (who is solving a sudoku), estimating how well certain strategies are understood. 
	The tutor, based on its model of the student, decides whether the benefits outweigh the costs of interaction with the student (i.e. proposing a move or explaining a step) when the student lacks knowledge. The student also has the ability to ask for further guidance; in other words, the two involved parties can take the initiative to engage in interaction. In comparison, in our approach, we do not aim to model the user, but rather assume a cost-function is specified that quantifies difficulties of derivations. At each point in time, we then suggest the simplest next derivation.''} 
%\tias{This sentence is broken, split it up. A bit more on how yes/no relevant welcome, or just copy paste how described in the revision?}

\begin{quoteit}
The nested explanations can be seen as a solver branching on a variable that is forced to backtrack after detecting inconsistency. However, as solvers need to branch deep in the search tree, I could imagine having multiple levels of nested explanations. I would have liked to see how the authors manage the depth of these nested explanations. A human could easily lose track of the reasoning. How does the algorithm prevent this behavior?
\end{quoteit}

\answer{Our work is targeted at finding small explanations. This reduces the chance that there are many levels of nested explanations, but it does indeed not prevent it. On our data, we have not observed multiple levels of nested explanations. In section 10 `Discussion, future work and conclusions', we added the discussion of the possibility of multiple levels, as well as that we haven't observed it, to the paper.} 
%\bart{We clarified this in the paper?}
% we zitten met stappen die we niet verder kunnen opsplitsen, voor moeilijkere applicaties zal dat niet het geval zijn en dan moeten we een manier vinden om daar slim mee om te gaan.
%\tias{our work is targetted to finding small explanations. This reduces the chance that there are many levels of nested explanations, but it does indeed not prevent it. On our data, we have not observed multiple levels of nested explanations. \textit{and then indeed something like} we added this discussion of the possibility of multiple levels, as well as that we haven't observed it, to the paper}

\begin{quoteit}
In the cost function, how did you obtain the magic number 5? How do 4 and 6 compare to 5? Some justification would be appreciated. 
I would replace 100 by M as it is done with linear programming. You can keep the definition: "The number M is taken here to be larger than any reasonable explanation size" and add "We use M=100 for our experiments".
\end{quoteit}

\answer{
	The number 100 and 5 are indeed ``magic constants''. We have, following this suggestion, replaced them by constants M (for the large one) and N. The parameters $M$ is taken here to be larger than any reasonable explanation size and $N$ to be relatively small in comparison. The optimal value to assign to these constants is something we do not know yet and that requires additional research. We added a discussion in our future work section in the paragraph on \emph{`what constitutes a good explanation'}: 
	
		`These observations suggest that further research into the question \emph{what constitutes an understandable explanation for humans} is needed with respect to using appropriate interpretability metrics \cite{hoffman2018metrics,rosenfeld2021better}'.
} 

%\tias{Perhaps also something about the 5? e.g. a value N larger than typically expected number of literals or smth?}

% > pseudo-randomly, kosten mogelijke avenue for future work => hinten bij introduceren van de cost in de paper dat het voor future work is.
\begin{quoteit}
The MUS solver tries to find a globally small set of constraints. Even if it does not succeed, a MUS solver aims for this goal. However, I do not think your greedy algorithm tries to achieve any globally optimal explanation.
Could that greedy algorithm reconsiders some choices in order to make better choices in the future?
\end{quoteit}

\answer{The current greedy algorithm looks for good candidate explanations by generating constraints of increasing costs for the current state of play. It does not reconsider previous choices for better future choices. This is partly discussed in future work: \emph{`Additional directions to produce easier-to-understand explanations would be \emph{(i)} to  optimize the sequence as a whole, rather than only individual steps,..'}. One could consider a beam-search, but that is beyond the scope of this work and would further increase computational cost.}
%\tias{Alt: something like 'one could consider a beam-search, but that is beyond the scope of this work and would further increase computational cost'}
% > explanation sequence optimaliseren + hergebruiken van SSes.

\begin{quoteit}
It is nice to see that everything was implemented in a system that reads the problem definition in natural language and outputs the explanations in natural language. However, the discussion about NLP does not bring much to the main contribution of the paper and triggers some questions.
%\end{quoteit}
%\emilio{Even though the paper covers how an NLP of the problem statement to a logic specification, the focus of the journal paper lies indeed in the explanation generation. Note that very recent work has been done on interpreting and solving logic grid puzzles using a deep neural network approach.}
%\emilio{
%	Jabrayilzade, Elgun, and Selma Tekir. "LGPSolver-Solving Logic Grid Puzzles Automatically." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 2020.}
%\begin{quoteit}
For instance, when you write: ``Since any POS-tagger can make mistakes, we make sure that all of the puzzle's entities are tagged as noun.'' How? Manually?
\end{quoteit}

\answer{The phrasing of the puzzles are sometimes ambiguous and can lead to mistakes during the process of POS-tagging. To deal with these situations, we post-process the POS-tagged words by applying a set of pre-defined rules to recognize ambiguous cases. We then manually verify and correct the assigned POS-tags.}
\emilio{Todo add description post-tagging postprocessing} 
%\tias{his remark is not just to know the answer, but also for you to make the paper better by specifying it. So ideally, you can add to this answer: 'we further clarified this in the paper'}
% Yes... :(

\begin{quoteit}
It would have been interesting to try the system on harder problems (harder than grid puzzles) to test the limits of the system.  Grid puzzles seem easy problems since you mention "we never encountered an explanation step that combined multiple clues".  I can understand that you restrict yourself to grid puzzles to better process the natural language. However, it would be interesting to test on other problems without handling the natural language.
\end{quoteit}

%\new{Hmmm... Not sure how to handle this one. }\\
\answer{We agree on this with the reviewer. It is on our agenda for future work.}

\begin{quoteit}
Overall, this is a good paper that deserves to be published. 

Typos

Line 64: "does not have *the* all the"

Definition 6: I think you mean $I_{i-1}
$

Section 6.2: "of a derived this."
\end{quoteit}


\section*{Reviewer \#2 - DETAILED COMMENTS}

\begin{quoteit}
	This paper presents a method for step-wise explanation of solutions to
	constraint satisfaction problems. The main idea is the use of a cost
	function that estimates the epistemic accessibilty of the 'next'
	explanation for an inference step. In this paper the authors use 'size'
	for cost as a simplification, noting that average/max can be used for
	sequences.  The method is agnostic to the particular solver techniques,
	but the authors also show how explanation for particular applications
	can be improved with new cost functions, providing a good case study in
	the form of logic grid puzzles. The paper is also accompanied by a
	demonstration website with two puzzles. This was very useful in
	grounding the main ideas of the paper.
	
	The interesting part of the paper is the use of a human-reasoning
	'trick': reasoning by contradiction. This produces what the authors
	call 'nested explanations'. The paper motivates using logic grid
	puzzles. The nested explanation concept and the application are novel
	as far as I know. The paper appears to be sound. CSP is outside of my
	area, but the description in first-order logic is straightforward
	enough to understand as the paper is very nicely written. The results
	and experimentation are interesting. The key result (Figure 5) is that
	nested explanations are often much less costly than their 'parent', but
	sometimes they are also almost as costly -- and there are very few in
	between.
	
	
	Section 7: The observations in this section and how they tie to
	explainining logic grid puzzles is really nicely presented and very
	interesting. I imagine this would be required a bit of work and
	analysis. This section is really very nice.
	
	\rule{0.9\textwidth}{0.4pt}
	
Section 5 is sorely missing an example of the more difficult explanation. THe explanation from the reasoning by contradiction is straightforward enough to understand for a lay reader, but there is no contraste to the more difficult case.
\end{quoteit}

\tempanswer{The clue used for the parent explanation has been highlighted. In fact, there are other nested sequences, but most sequences are composed of many nested explanations. While this case is relatively straightforward, it is a good example of how nested sequences can even improve the understanding of easier steps.}

\bart{I thought we discussed this and planned to add another example. That never hurts. Is there any good reason \emph{not} to do it?}
\emilio{There is a whole image for it. Do you mean a textual description or working out the cost steps etc...?}
\tias{the current answer is not very satisfying in any case, it sidesteps the question}
\emilio{I think the explanation is not that obvious... to be discussed at next meeting}

\begin{quoteit}
It is not clear to me what happens in Algorithm 5 when f(E', S', N') $>=$ f(E, S, N). Does not explanation get stored at all for this step because it is too complicated? 
\end{quoteit}

\answer{We have clarified this in \emph{6.4 Searching nested expanations} of the paper:
	
		``While Algorithm 5 tries to find a nested explanation sequence for each derived fact, it will not find one for each fact due to the if-check at Line 8. 
		This check is present to avoid that the nested explanation is as difficult or harder than the high-level step it aims to clarify (also see Definition 10, where the third bullet point explicitly forbids such nested justifications).  
		This check can kick in for two different reasons. The first reason is that the explanation step at the main level is simply too simple to be further broken up in pieces. For instance the explanation of Figure 1 is of that kind: it uses a single bijectivity constraint with a single previously derived fact. Breaking this derivation up in strictly smaller parts would thus not be helpful. But this phenomenen can also occur for very difficult steps: sometimes the best nested explanation of a difficult explanation step contains a step that is as difficult or harder than the high-level step itself. In that case, this is a sign that reasoning by contradiction is not simplifying matters in this step and other methods should be explored to further explain it.''}

\begin{quoteit}
The demo website is great. Thanks for taking the time to put that together. My only feedback here is that when transitivity constraints, bijectivity, etc., are added, the explanation breaks down a bit. It could be simply that I didn't really  understand what the bijectivity constraint was applied to for this particular puzzle, so changing the text here would help I guess?
\end{quoteit}

\answer{We thank the reviewer for their suggestion and will incorporate this into the demo on the website.} 
%\tias{Why not now? What if we forget?}
%\emilio{The implementation will take some time, I will take care of this in the coming weeks}

\begin{quoteit}
I very much encourage the authors to conduct some human-behavioural studies using this in future. I believe there is enough in this paper already that it is suitable, but the method (and in particular the inclusion of the observations in Section 7 for the cost function), while interesting, are not really evaluated from the perspective of whether they do what they aim to do: help people understand CSP solutions. 
\end{quoteit}

\answer{We thank the reviewer for their suggestion and will look into it. We have included this idea as future work.} 
%\tias{and will look into it? the idea is that we improve our work and understanding based on their constructive comments...}

%\emilio{We will include this suggestion as potential follow-up work.}

\begin{quoteit}
As well as looking at some studies in this space, a good place to start is Hoffman et al.,: "Metrics for explainable AI: Challenges and prospects" https://arxiv.org/pdf/1812.04608 
\end{quoteit}

\answer{We have included this reference in the future works section.}
\begin{quoteit}
Very minor things:

- Page 2: "have [the] all the knowledge" -- remove the first 'the'

- Algorithm 5, line 9: has two statements on one line; maybe easier to
read if broken into two lines

- Figure 5: The x-axis has 1, then 9, then 2-8. Is this intentional? I
don't see a reason why they would be out of order all of a sudden (not
that the ordering matters, but it just seems strange)
\end{quoteit}

\Revision{Figure 5's x-axis has been adapted with the same ordering as the other puzzles.}

%\bart{Meta-comment. Do not abuse existing latex comments to give them a different meaning. Making a newcommand answer is not much more work than just using emilio everywhere, whether it is a change, an answer, or a question from you... }

\bibliographystyle{elsarticle-num}
\bibliography{ref,mybibfile}
\end{document}
