{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# convert json file to BOS prolog input\n",
    "#\n",
    "# be sure to install 'python3-nltk'\n",
    "# and run: 'pip3 install pattern'\n",
    "# then run once: \n",
    "import nltk #; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger'); nltk.download('wordnet')\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os.path\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "from pattern.en import pluralize, singularize, lemma\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger'); nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem(zebra, problem(5, 5, [\n",
      "        \"there are five houses \",\n",
      "        \"the englishman lives in the red house \",\n",
      "        \"the spaniard owns the dog \",\n",
      "        \"coffee is drunk in the green house \",\n",
      "        \"the ukrainian drinks tea \",\n",
      "        \"the green house is immediately to the right of the ivory house \",\n",
      "        \"the old gold smoker owns snails \",\n",
      "        \"kools are smoked in the yellow house \",\n",
      "        \"milk is drunk in the middle house \",\n",
      "        \"the norwegian lives in the first house \",\n",
      "        \"the man who smokes chesterfields lives in the house next to the man with the fox \",\n",
      "        \"kools are smoked in the house next to the house where the horse is kept \",\n",
      "        \"the lucky strike smoker drinks orange juice \",\n",
      "        \"the japanese smokes parliaments \",\n",
      "        \"the norwegian lives next to the blue house \"\n",
      "                     ], [\n",
      "    pn([blue]),\n",
      "    pn([chesterfields]),\n",
      "    pn([coffee]),\n",
      "    pn([dog]),\n",
      "    pn([englishman]),\n",
      "    pn([fox]),\n",
      "    pn([green]),\n",
      "    pn([horse]),\n",
      "    pn([ivory]),\n",
      "    pn([japanese]),\n",
      "    pn([kools]),\n",
      "    pn([lucky_strike]),\n",
      "    pn([milk]),\n",
      "    pn([norwegian]),\n",
      "    pn([old_gold]),\n",
      "    pn([orange_juice]),\n",
      "    pn([parliaments]),\n",
      "    pn([red]),\n",
      "    pn([snails]),\n",
      "    pn([spaniard]),\n",
      "    pn([tea]),\n",
      "    pn([ukranian]),\n",
      "    pn([water]),\n",
      "    pn([yellow]),\n",
      "    pn([zebra]),\n",
      "    noun([drink], [drinks]),\n",
      "    noun([gold], [golds]),\n",
      "    noun([house], [houses]),\n",
      "    noun([juice], [juices]),\n",
      "    noun([life], [lives]),\n",
      "    noun([man], [men]),\n",
      "    noun([orange], [oranges]),\n",
      "    noun([right], [rights]),\n",
      "    noun([smoke], [smokes]),\n",
      "    noun([smoker], [smokers]),\n",
      "    noun([strike], [strikes]),\n",
      "    tv([is], [be]),\n",
      "    tv([is, kept], [be]),\n",
      "    tv([are], [be]),\n",
      "    tv([house], [house]),\n",
      "    tv([is, drunk], [be]),\n",
      "    tv([owns], [own]),\n",
      "    tv([are, smoked], [be]),\n",
      "    tvPrep([drunk], [in], [drink], [todooo]),\n",
      "    tvPrep([lives], [in], [live], [todooo]),\n",
      "    tvPrep([smoked], [in], [smoke], [todooo])\n",
      "                     ])).\n"
     ]
    }
   ],
   "source": [
    "name,out = convert(\"/home/crunchmonster/Documents/VUB/Research/NLP/holygrail/notebooks/zebra.json\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mylemma(verb):\n",
    "    # to catch special cases...\n",
    "    if not isinstance(verb, str):\n",
    "        return verb\n",
    "    if verb == \"'s\": # Claudia's black coffee, TODO?\n",
    "        return verb\n",
    "    if verb == \"n't\": # wasn't from, TODO?\n",
    "        return verb\n",
    "    return lemma(verb) # pattern.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(fname):\n",
    "    with open(fname, 'r') as ffile:\n",
    "        data = json.load(ffile)\n",
    "\n",
    "        name = data['title'].lower().replace(' ','_')\n",
    "        nr_types = len(data['types'])\n",
    "        nr_domsize = len( next(iter(data['types'].values())) ) # any element\n",
    "\n",
    "        (clues, lexicon) = get_lexicon(data)\n",
    "        out = \"problem({}, problem({}, {}, {}, {})).\".format(\n",
    "                name,\n",
    "                nr_types,\n",
    "                nr_domsize,\n",
    "                pp_clues(clues),\n",
    "                lexicon)\n",
    "        return (name, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_clues(clues):\n",
    "    # pretty printing stuff\n",
    "    clues = [\"        \\\"{}\\\"\".format(clue) for clue in clues]\n",
    "    return \"[\\n\"+\",\\n\".join(clues)+\"\\n                     ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_clues(clues_pos):\n",
    "    # cleaning, some nouns may still have a '.' in them, which is problematic for prolog\n",
    "    # cleaning, replace \"n't\" by \"not\"\n",
    "    for clue_pos in clues_pos:\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            if pos.startswith('NN') and '.' in word:\n",
    "                clue_pos[i] = (word.replace('.',''), pos)\n",
    "            if word == \"n't\":\n",
    "                clue_pos[i] = (\"not\", pos)\n",
    "            if word != \".\" and \".\" in word: # words with . like 'mrs.'\n",
    "                clue_pos[i] = (word.replace(\".\", \"\"), pos)\n",
    "    return clues_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_clues_pns(clues_pos, pns):\n",
    "    whitelist = frozenset(flatten(pns)) # do not repeat words part of a pn\n",
    "    # make sure al pns are tagged as 'NN'\n",
    "    for clue_pos in clues_pos:\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            if word in whitelist and not pos.startswith('NN'):\n",
    "                clue_pos[i] = (word, 'NNPN')\n",
    "    return clues_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_clues_nns(clues_pos, nns):\n",
    "    # cleaning: make numbers integer\n",
    "    for clue_pos in clues_pos:\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            # CD (cardinal number) -- remove comma or dot\n",
    "            if pos == 'CD':\n",
    "                # this may not be sufficiently robust with fractional numbers (e.g, 0.9 vs 0.90)\n",
    "                word = word.replace(\".\",\"\")\n",
    "                word = word.replace(\",\",\"\")\n",
    "                if word in nns:\n",
    "                    # tag it a NNN if in the names list (numeric)\n",
    "                    clue_pos[i] = (word, 'NNN')\n",
    "                else:\n",
    "                    clue_pos[i] = (word, pos)\n",
    "    return clues_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pn_nn(data_types):\n",
    "    # proper nouns (numbers separately)\n",
    "    pns = set()\n",
    "    nns = set() # numeric ones\n",
    "    for (category, names) in data_types.items():\n",
    "        for n in names:\n",
    "            if isinstance(n, (int,float)):\n",
    "                nns.add(n)\n",
    "            else:\n",
    "                pn = n.lower()\n",
    "                if ' ' in pn:\n",
    "                    pn = tuple(pn.split(' ')) # must be hashable\n",
    "                pns.add(pn)\n",
    "    return (pns, nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppns(clues_pos, data_types, pns):\n",
    "    # check for ppn's\n",
    "    # extract [DT pn NN] triples\n",
    "    pn_triples = dict((pn,[]) for pn in pns)\n",
    "    for clue_pos in clues_pos:\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            word = word.lower()\n",
    "            if word in pns:\n",
    "                if not pos.startswith('NN'):\n",
    "                    # make sure it is correctly tagged\n",
    "                    clue_pos[i] = (word, 'NN')\n",
    "                # check for triple\n",
    "                if i > 0 and clue_pos[i-1][1] == 'DT' \\\n",
    "                and i+1 < len(clue_pos) and clue_pos[i+1][1] == 'NN':\n",
    "                    pn_triples[word].append( (clue_pos[i-1][0],clue_pos[i+1][0]) )\n",
    "    # check for each type, use most common triple\n",
    "    ppns = set() # XXX deprecated, remove later\n",
    "    ppns_dict = dict() # pn -> triple\n",
    "    for (category, names) in data_types.items():\n",
    "        triples = []\n",
    "        for name in names:\n",
    "            if not isinstance(name, (int,float)):\n",
    "                name = name.lower()\n",
    "                if name in pn_triples:\n",
    "                    triples += pn_triples[name]\n",
    "        if len(triples) > 0:\n",
    "            firstdist = nltk.FreqDist([phrase[0] for phrase in triples])\n",
    "            lastdist = nltk.FreqDist([phrase[-1] for phrase in triples])\n",
    "            for name in names:\n",
    "                ppns.add( (firstdist.max(),name.lower(),lastdist.max()) ) # XXX depr\n",
    "                ppns_dict[name.lower()] = (firstdist.max(),name.lower(),lastdist.max())\n",
    "    # fix the less common triple ppn usages in the clues...\n",
    "    j = 0 # old style because we will restart for j\n",
    "    while j < len(clues_pos):\n",
    "        clue_pos = clues_pos[j]\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            if word in ppns_dict:\n",
    "                triple = ppns_dict[word]\n",
    "                start = i-1 # standard case\n",
    "                stop = i+1 # standard case\n",
    "                if stop >= len(clue_pos):\n",
    "                    # not sure... a bug probably?\n",
    "                    continue\n",
    "                if clue_pos[start][0] == triple[0] and clue_pos[stop][0] == triple[2]:\n",
    "                    continue # all good\n",
    "                # special case: -1 is not a DT, but -2 is\n",
    "                if start-1 >= 0 and clue_pos[start][1] != 'DT' and clue_pos[start-1][1] == 'DT':\n",
    "                    start = i-2\n",
    "                # special case: +1 is not the 3-NN, but +2 is\n",
    "                if stop+1 < len(clue_pos) and clue_pos[stop][0] != triple[2] and clue_pos[stop+1][0] == triple[2]:\n",
    "                    stop = i+2\n",
    "                triple_pos = [ (triple[0], 'DT'), (triple[1], 'NN'), (triple[2], 'NN') ]\n",
    "                clues_pos[j] = clue_pos[:start] + triple_pos + clue_pos[stop+1:]\n",
    "                j = j - 1 # restart j\n",
    "        j = j + 1\n",
    "\n",
    "    # check for double meanings\n",
    "    #poscounts = nltk.ConditionalFreqDist([tag for tags in clues_pos for tag in tags])\n",
    "    #poscounts_ambigu = [(x,poscounts[x]) for x in poscounts if len(poscounts[x]) > 1]\n",
    "    #print(\"Ambiguous:\",poscounts_ambigu)\n",
    "    return ppns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(clues_pos, pns):\n",
    "    blacklist = frozenset(flatten(pns)) # do not repeat words part of a pn\n",
    "    \n",
    "    # extra nouns that are not our proper nouns\n",
    "    nouns = set()\n",
    "    for clue_pos in clues_pos:\n",
    "        for (word, pos) in clue_pos:\n",
    "            word = word.lower()\n",
    "            if word in blacklist:\n",
    "                continue # skip\n",
    "            if pos.startswith('NNS'):\n",
    "                # plural\n",
    "                nouns.add( (singularize(word),word) )\n",
    "            elif pos.startswith('NN'):\n",
    "                # singular\n",
    "                nouns.add( (word,pluralize(word)) )\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(clues_pos, pns):\n",
    "    # fix pos tags with wordnet's verb list\n",
    "    # this is a hack and due to an insufficient POS tagger...\n",
    "    # https://stackoverflow.com/questions/28033882/determining-whether-a-word-is-a-noun-or-not\n",
    "    #\n",
    "    # I think we should be more careful, e.g. when a sentence does not have a VB only?\n",
    "    wordnet_verbs = frozenset(x.name().split('.', 1)[0] for x in wn.all_synsets('v'))\n",
    "    for clue_pos in clues_pos:\n",
    "        if not any(pos.startswith('VB') for (word, pos) in clue_pos):\n",
    "            for (i,(word, pos)) in enumerate(clue_pos):\n",
    "                if not pos.startswith('VB') and mylemma(word) in wordnet_verbs:\n",
    "                    clue_pos[i] = (word, 'VBWN')\n",
    "\n",
    "    tvs = set()\n",
    "    tvpreps = set()\n",
    "    for clue_pos in clues_pos:\n",
    "        for (i,(word, pos)) in enumerate(clue_pos):\n",
    "            if pos.startswith('VB'):\n",
    "                if i+1 < len(clue_pos) and clue_pos[i+1][1] == 'IN':\n",
    "                    # tvprep: verb with preposition\n",
    "                    lemm = mylemma(word)\n",
    "                    tvpreps.add( (word, clue_pos[i+1][0], lemm) )\n",
    "                elif i+1 < len(clue_pos) and clue_pos[i+1][1] == 'VBN':\n",
    "                    # tv-2-sized, e.g. 'was prescribed'\n",
    "                    lemm = mylemma(word)\n",
    "                    word2 = clue_pos[i+1][0]\n",
    "                    tvs.add( ((word,word2), lemm) )\n",
    "                elif pos == 'VBN' and i > 0 and clue_pos[i-1][1].startswith('VB'):\n",
    "                    # previous was also a verb, so this one already used as tv-2-sized\n",
    "                    pass\n",
    "                else:\n",
    "                    # tv: transitive verb\n",
    "                    lemm = mylemma(word)\n",
    "                    tvs.add( (word, lemm) )\n",
    "\n",
    "    # do not output builtin tvs\n",
    "    blacklist = [\"do\", \"have\"]\n",
    "    blacklist += [\"'s\"] # weird weird\n",
    "    # do not output if it has a tvprep either\n",
    "    blacklist += [lemm for (word, prep, lemm) in tvpreps]\n",
    "    # remove blacklisted\n",
    "    for (word, lemm) in list(tvs):\n",
    "        if lemm in blacklist:\n",
    "            tvs.remove( (word, lemm) )\n",
    "\n",
    "    # check for duplicates in tvprep, try to resolve\n",
    "    tvprep_dict = dict()\n",
    "    for (word, prep, lemm) in tvpreps:\n",
    "        key = (lemm, prep)\n",
    "        if not key in tvprep_dict:\n",
    "            tvprep_dict[key] = [word]\n",
    "        else:\n",
    "            tvprep_dict[key].append(word)\n",
    "    for ((lemm,prep), lst) in tvprep_dict.items():\n",
    "        if len(lst) > 1:\n",
    "            # if duplicate and lemm is also as a first one, remove that one\n",
    "            if lemm in lst:\n",
    "                tvpreps.remove( (lemm, prep, lemm) )\n",
    "\n",
    "    return (tvs, tvpreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon(data, jsondemo=False):\n",
    "    # do the nlp stuff on lowercase sentences\n",
    "    # http://www.nltk.org/book/ch07.html\n",
    "    clues_token = [nltk.word_tokenize(clue.lower()) for clue in data['clues']]\n",
    "    clues_pos = [nltk.pos_tag(clue) for clue in clues_token]\n",
    "\n",
    "    # some cleaning\n",
    "    clues_pos = clean_clues(clues_pos)\n",
    "\n",
    "    # get proper nouns and number nouns\n",
    "    (pns, nns) = get_pn_nn(data['types'])\n",
    "    clues_pos = clean_clues_pns(clues_pos, pns)\n",
    "    clues_pos = clean_clues_nns(clues_pos, nns)\n",
    "\n",
    "    if jsondemo:\n",
    "        # POS tag printing for demo\n",
    "        print(json.dumps(clues_pos))\n",
    "\n",
    "    ppns = {} #get_ppns(clues_pos, data['types'], pns)\n",
    "\n",
    "    nouns = get_nouns(clues_pos, pns)\n",
    "\n",
    "    (tr_verbs, two_word_tr_verbs) = get_verbs(clues_pos, pns)\n",
    "    (tvs, tvpreps) = get_verbs(clues_pos, pns)\n",
    "                    \n",
    "    \n",
    "    # refactored up to here\n",
    "    clues_revised = []\n",
    "    for clues in clues_pos:\n",
    "        target = []\n",
    "        for i,item in enumerate(clues):# enumerate(nltk.pos_tag(clues_token_clean[6])):\n",
    "            word = item[0].lower()\n",
    "\n",
    "            # noun\n",
    "            if word in pns:\n",
    "                target.append( (word, 'pn') )\n",
    "            elif word in flatten(nouns) and word not in flatten(ppns):\n",
    "                target.append( (word, 'noun') )\n",
    "            elif word in flatten(nouns) and word in flatten(ppns):\n",
    "                if len(target) >= 2 and target[-2][1] == 'DT':\n",
    "                    for ppn in ppns:\n",
    "                        if target[-1][0] in ppn:\n",
    "                            del target[-2:]\n",
    "                            target.append( (ppn, 'ppn') )\n",
    "                            break\n",
    "                elif len(target) >= 3 and target[-3][1] == 'DT':\n",
    "                    for ppn in ppns:\n",
    "                        if target[-1][0] in ppn or target[-2][0] in ppn:\n",
    "                            del target[-3:]\n",
    "                            target.append( (ppn, 'ppn') )\n",
    "                            break\n",
    "            # verb\n",
    "            elif item[1].startswith('VB'):\n",
    "                if len(target) >= 2 and target[-1][1].startswith('VB'):\n",
    "                    target.append( ((target[-1][0], word), 'tv') )\n",
    "                    del target[-2]\n",
    "                elif word in flatten(tvs):\n",
    "                    target.append( (word, 'tv') )\n",
    "                else:\n",
    "                    target.append(item)\n",
    "\n",
    "            # prep\n",
    "            elif item[1] == 'IN' and i > 1:\n",
    "                if len(target) >= 2 and target[-1][1].startswith('VB'):\n",
    "                    target.append( ((target[-1][0], word), 'tvPrep') )\n",
    "                    del target[-2]\n",
    "                else:\n",
    "                    target.append(item)\n",
    "\n",
    "            else:\n",
    "                target.append(item)    \n",
    "                \n",
    "        clues_revised.append(target)\n",
    "    #print(\"\\n\".join(map(str,clues_revised)))\n",
    "\n",
    "    # reconstruct sentences from revised clues\n",
    "    clues_new = []\n",
    "    for clue in clues_revised:\n",
    "        clue_str = \"\"\n",
    "        for (word, tag) in clue:\n",
    "            if isinstance(word, (list,tuple)):\n",
    "                # multiple words\n",
    "                clue_str += \" \".join(word)\n",
    "            else:\n",
    "                clue_str += word\n",
    "            clue_str += \" \"\n",
    "        clues_new.append(clue_str)\n",
    "    #print(clues_new)\n",
    "    \n",
    "    # finding tvGap\n",
    "    punctuations = frozenset(string.punctuation)\n",
    "    tvgap_phrases = []\n",
    "    for clues in clues_revised:\n",
    "        for i,item in enumerate(clues):\n",
    "            if item[-1].startswith('tv') and len(clues) > i+1 and (clues[i+1][-1] == 'pn' or clues[i+1][-1] == 'ppn'):\n",
    "                # till end of sentence, check for punctuation\n",
    "                start = i+1\n",
    "                stop = len(clues)\n",
    "                for j in range(start,stop):\n",
    "                    if clues[j][0] in punctuations:\n",
    "                        stop = j\n",
    "                        break\n",
    "                if clues[start:stop]: # rest of the phrase is not empty, add it\n",
    "                    tvgap_phrases.append( [item] + clues[start:stop] )\n",
    "    \n",
    "    # TODO, I broke tvgap detection in tutorial...\n",
    "    # dictionary to find phrase frequency\n",
    "    cnt = {}\n",
    "    for tvgap in tvgap_phrases:\n",
    "        t = tuple(tvgap) # for dict\n",
    "        if t in cnt:\n",
    "            cnt[t] +=1\n",
    "        else:\n",
    "            cnt[t] = 1 \n",
    "            \n",
    "    tvGap_list = []\n",
    "    for tvgap,val in cnt.items():\n",
    "        # tvgap = [ (words, \"TV\"), (rest, tag), (of, tag), ([multi,part,sentence],tag) ]\n",
    "        if val>1:\n",
    "            tv = tvgap[0][0]\n",
    "            gapwords = []\n",
    "            for item in tvgap[1:]:\n",
    "                for word in item[:-1]:\n",
    "                    gapwords.append(word)\n",
    "            # find current tv and its lemma, remove from tv\n",
    "            thelemma = \"noooone\"\n",
    "            for (v,v2) in list(tvs): # list() because we modify it\n",
    "                if v == tv:\n",
    "                    thelemma = v2\n",
    "                    tvs.remove( (v,v2) )\n",
    "            tvGap_list.append( (tv,gapwords,thelemma) )\n",
    "    \n",
    "    # for printing, remove ppns from pns\n",
    "    for (pre,pn,post) in ppns:\n",
    "        if pn in pns:\n",
    "            pns.remove(pn)\n",
    "        #else: a bug probably due to pn with a space hack (e.g. 'van wert')\n",
    "\n",
    "\n",
    "    # dictionary from word to lexicon element (for printing)\n",
    "    lexdict = dict()\n",
    "    \n",
    "    # pn's\n",
    "    pns_str = []\n",
    "    for pn in pns:\n",
    "        if isinstance(pn, tuple):\n",
    "            pn = \", \".join(pn)\n",
    "        pn_str = f\"    pn([{pn}])\"\n",
    "        lexdict[pn] = pn_str\n",
    "        pns_str.append( pn_str )\n",
    "    pns_str = sorted(pns_str)\n",
    "\n",
    "    # ppn's\n",
    "    ppns_str = []\n",
    "    for (a,b,c) in sorted(ppns):\n",
    "        ppn_str = \"    ppn([{}, {}, {}])\".format(a,b,c)\n",
    "        for e in (a,b,c):\n",
    "            lexdict[e] = ppn_str\n",
    "        ppns_str.append(ppn_str)\n",
    "\n",
    "    # noun's\n",
    "    nouns_str = []\n",
    "    for (s,p) in sorted(nouns):\n",
    "        noun_str = \"    noun([{}], [{}])\".format(s,p)\n",
    "        lexdict[s] = noun_str\n",
    "        lexdict[p] = noun_str\n",
    "        nouns_str.append(noun_str)\n",
    "\n",
    "    # tv's and variants\n",
    "    tv_str = []\n",
    "    for (v,v2) in tvs:\n",
    "        if isinstance(v, (tuple,list)):\n",
    "            v = \", \".join(v)\n",
    "        tv_s = f\"    tv([{v}], [{v2}])\"\n",
    "        lexdict[v] = tv_s\n",
    "        lexdict[v2] = tv_s\n",
    "        tv_str.append( tv_s )\n",
    "    \n",
    "    tvprep_str = []\n",
    "    for (v,p,v2) in sorted(tvpreps):\n",
    "        tvprep_s = \"    tvPrep([{}], [{}], [{}], [todooo])\".format(v,p,v2)\n",
    "        for e in (v,p,v2):\n",
    "            lexdict[e] = tvprep_s\n",
    "        tvprep_str.append(tvprep_s)\n",
    "    tvgap_str = []\n",
    "    for (v,gap,v2) in tvGap_list:\n",
    "        if isinstance(v, (tuple, list)):\n",
    "            v = \", \".join(v)\n",
    "        one = \"[{}]\".format(v) # this too may hide bug...\n",
    "        two = \"[{}]\".format(\", \".join(flatten(gap))) # XXX, the flatten may hide a bug\n",
    "        mystr = \"    tvGap({}, {}, [{}])\".format(one, two, v2)\n",
    "        for e in (one,two,v2):\n",
    "            lexdict[e] = mystr\n",
    "        tvgap_str.append(mystr)\n",
    "    tvgap_str = sorted(tvgap_str)\n",
    "\n",
    "    if jsondemo:\n",
    "        # per-sentence lexicon printing for demo\n",
    "        lexclues = []\n",
    "        print(lexdict)\n",
    "        for clue in clues_new:\n",
    "            lexclue = []\n",
    "            for word in clue.split(' '):\n",
    "                if word in lexdict:\n",
    "                    lexclue.append(lexdict[word].strip())\n",
    "                else:\n",
    "                    # special case for numbers\n",
    "                    try:\n",
    "                        v = int(word)\n",
    "                        lexclue.append(f\"number({v})\")\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            lexclues.append(lexclue)\n",
    "        print(json.dumps(lexclues))\n",
    "            \n",
    "    return (clues_new,\n",
    "           \"[\\n\"+\\\n",
    "           \",\\n\".join(pns_str + ppns_str + nouns_str + tv_str + tvprep_str + tvgap_str)+\\\n",
    "           \"\\n                     ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/47432632/flatten-multi-dimensional-array-in-python-3\n",
    "def flatten(something):\n",
    "    if isinstance(something, (list, tuple, set, range)):\n",
    "        for sub in something:\n",
    "            yield from flatten(sub)\n",
    "    else:\n",
    "        yield something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preamble():\n",
    "    return \"\"\"\n",
    ":- module(problemHolyGrail, [problem/2]).\n",
    "\n",
    "problem(tias, problem(4, 4, [\n",
    "        % \"The 4 people were Tatum, the patient who was prescribed enalapril, the employee with the $54,000 salary, and the owner of the purple house\",\n",
    "% CHANGED TO: ( \"with the salary\")\n",
    "        \"The 4 people were tatum, the patient who was prescribed enalapril, the employee who earns 54000, and the owner of the purple house\",\n",
    "        \"The patient who was prescribed enalapril is not heather\",\n",
    "        \"The patient who was prescribed ramipril is not annabelle\",\n",
    "        \"kassidy earns less than heather\",\n",
    "        \"The owner of the blue house earns more than kassidy\",\n",
    "%%    \"Of tatum and annabelle, one earns 144000 per year and the other lives in the cyan colored house\",\n",
    "%% CHANGED TO: (drop: colored)\n",
    "        \"Of tatum and annabelle, one earns 144000 per year and the other lives in the cyan house\",\n",
    "%%        \"Either the employee with the 144000 salary or the employee with the 158000 salary lives in the blue colored house\",\n",
    "%% CHANGED TO: (drop colored, change the ...salara) \n",
    "        \"Either the employee who earns 144000  or the employee who earns 158000 lives in the blue house\",\n",
    "        \"The owner of the lime house was prescribed enalapril for their heart condition\",\n",
    "%%        \"The employee with the 144000 salary was prescribed benazepril for their heart condition\"\n",
    "%% CHANGED TO:\n",
    "        \"The employee who earns 144000 was prescribed benazepril for their heart condition\"\n",
    "                     ], [\n",
    "                        noun([patient], [patients]),\n",
    "                        noun([person], [people]),\n",
    "                        noun([year], [years]),\n",
    "                        noun([employee], [employees]),\n",
    "                        noun([salary], [salaries]),\n",
    "                        noun([owner], [owners]),\n",
    "                        pn([tatum]),\n",
    "                        pn([annabelle]),\n",
    "                        pn([heather]),\n",
    "                        pn([kassidy]),\n",
    "                        pn([benazepril]),\n",
    "                        pn([enalapril]),\n",
    "                        pn([ramipril]),\n",
    "                        pn([fosinopril]),\n",
    "                        prep([of]),\n",
    "                        ppn([the, blue, house]),\n",
    "                        ppn([the, lime, house]),\n",
    "                        ppn([the, cyan, house]),\n",
    "                        ppn([the, purple, house]),\n",
    "                        tv([owns], [own]),\n",
    "                        tvGap([earns], [per, year], [earn]),\n",
    "                        tvGap([was, prescribed], [for, their, heart, condition], [prescribe]),\n",
    "                        tvPrep([lives], [in], [live], [lived])\n",
    "                     ])).\n",
    "\n",
    "problem(p2_types, problem(4,5, [\n",
    "                        \"Of the contestant who scored 41 points and the person who threw the white darts, one was from Worthington and the other was Ira\",\n",
    "                        \"Bill was from Mount union\",\n",
    "                        \"Ira scored 21 points higher than the contestant from Worthington\",\n",
    "                        \"Oscar scored somewhat higher than the player who threw the orange darts\",\n",
    "                        \"The contestant from Mount union threw the black darts\",\n",
    "                        \"Pedro didn't finish with 55 points\",\n",
    "                        \"The player who threw the red darts was either Colin or the contestant who scored 48 points\",\n",
    "                        \"Of the contestant who scored 41 points and the person who threw the orange darts, one was from Gillbertville and the other was from Worthington\",\n",
    "                        \"Ira scored 7 points lower than the player from Lohrville\"\n",
    "        ], [\n",
    "                        noun([contestant], [contestants]),\n",
    "                        noun([person], [persons]),\n",
    "                        noun([player], [players]),\n",
    "                        noun([point], [points]),\n",
    "                        pn([bill], A),\n",
    "                        pn([colin], A),\n",
    "                        pn([ira], A),\n",
    "                        pn([oscar], A),\n",
    "                        pn([pedro], A),\n",
    "                        pn([mount, union], B),\n",
    "                        pn([gillbertville], B),\n",
    "                        pn([lohrville], B),\n",
    "                        pn([worthington], B),\n",
    "                        pn([yorktown], B),\n",
    "                        ppn([the, black, darts], C),\n",
    "                        ppn([the, orange, darts], C),\n",
    "                        ppn([the, red, darts], C),\n",
    "                        ppn([the, white, darts], C),\n",
    "                        ppn([the, yellow, darts], C),\n",
    "                        tv([threw], [throw]),\n",
    "                        tv([scored], [score]),\n",
    "                        tvPrep([finishes], [with], [finish], [finished]),\n",
    "                        prep([from])\n",
    "        ])).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assert (len(sys.argv) == 2), \"Expecting 1 argument: the json file or -a\"\n",
    "\n",
    "    if sys.argv[1] != '-a':\n",
    "        (name, out) = convert(sys.argv[1])\n",
    "        print(out)\n",
    "    else:\n",
    "        # print all\n",
    "        print(preamble())\n",
    "        allfiles = glob.glob('*.json')\n",
    "        allnames = []\n",
    "        for fname in allfiles:\n",
    "            (name, out) = convert(fname)\n",
    "            allnames.append(name)\n",
    "            print(out)\n",
    "            print(\"\\n\")\n",
    "        print(\"% \"+\", \".join(allnames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
