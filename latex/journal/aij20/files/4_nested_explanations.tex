From results in previous work \cite{ecai/BogaertsGCG20}, we observed that, for \textit{difficult} puzzles, some explanation steps were still quite hard to understand.
An example is depicted in Figure \ref{fig:pasta_diff}.
It uses a complicated (disjunctive) clue (``The person who ordered Rotini is either the person who paid \$8 more than Damon or the person who paid \$8 less than Damon'' ), in combination with three previously derived facts to derive that Farfalle does not cost \$8.
The reasoning behind the scenes can be explained by contradiction as follows: if Farfalle did cost \$8, then it does not cost \$16, hence the only possible pasta that can still cost \$16 would be Rotini.
However, since Damon did not take Farfelle (which we assumed costs \$8), Damon did not pay \$8; this is in contradiction with the highlighted clue. Hence Farfalle cannot cost \$8.

While equally straightforward for a computer, understanding such an indirect proof, using contradiction, in a single step is notably harder for a person than the more common forward reasoning explanations our tool generates.
%  Hence, we expanded our tool to -- on demand -- generate a \textit{nested} explanation of this contradiction reasoning.
% It turns out from preliminary analysis of the generated explanation sequence that some steps are still too hard to understand as they combine different constraints and/or multiple clues. 
\bart{DIE LAATSTE SCREENSHOT IS NIET JUIST!
 Dat lijkt een heel oude versie... Kan je aub updaten met de laatste zoals op https://bartbog.github.io/zebra/pasta/ Ook best de ``inconsistency'' niet bovenop een gebruikte clue}
\begin{figure}[t!]
    \includegraphics[width=\linewidth]{figures/incosistency.jpg}
    \caption{A difficult explanation step}\label{fig:pasta_diff}
\end{figure}

In this section, we extend the explanation-production problem with the purpose of refining those explanations that are too complex and taking inspiration from counterfactual reasoning.
Thus, our problem becomes an explanation-generating problem with 2 levels of abstractions: ``regular'' explanations and ``lower-level'' \textit{nested-explanations}.

\myparagraph{Nested explanation sequence production}

% To tackle complex inference steps with the lense of counterfactual reasoning, one  \textit{``Can I find another easier way to explain this step using the same facts and constraints by negating the newly derived fact and finding an inconsistency ?''} 


% For the definition of \textit{inconsistency}, we refer back to definition \ref{def:consistent}.


The generation of the explanation sequence, as formally defined in section 4, is initially guided by the cost function $f(E, S, N)$, a proxy for the mental-effort of the explanation-step. To tackle the complex inference steps generated in the explanation sequence, we specify the concept of \emph{nested-explanation} as follows:

\begin{definition}
    Given a non-redundant explanation $(E_i, S_i, N_i)$, let $n_i \in N_i$ be a newly derived fact and partial interpretation  $I_0' = \{ \neg n_i \wedge E_i \}$ , the \textbf{nested-explanation} problem consists of finding a non-redundant explanation sequence that leads to an inconsistency
    \[\langle \ (I_0',(\emptyset,\emptyset,\emptyset)),\ (I_1',(E_1',S_1',N_1')), \dots ,\ (I_n',(E_n',S_n',N_n')) \ \rangle\]
    such that
    \begin{itemize}
        \item $\forall \ (E_i',S_i',N_i') : f(E_i',S_i',N_i') \leq f(E_i, S_i, N_i)$ and
        \item a predefined aggregate over the sequence $\left(f(E_i',S_i',N_i')\right)_{i\leq n}$ is minimised.
    \end{itemize}
\end{definition}

Put differently, for every newly derived fact of a given explanation step, we look for an \emph{nested} explanation-sequence such that every nested explanation-step is easier than the original explanation step.



% Ideas : 
% \begin{itemize}
%     \item During analysis of sequence of reasoning steps too hard/ complex to understand 
%     \item 2 levels of abstraction
%     \item Refine explanations using counterfactual reasoning
%     \item 
% \end{itemize}

% We introduce a second
