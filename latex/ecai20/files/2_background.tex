We will assume familiarity with \emph{typed first-order logic} and \emph{typed second-order logic}.  
A logical vocabulary consists of a set of type symbols, typed constant symbols, and relation symbols with associated type signature (i.e., each relation symbol is typed $T_1\times \dots \times T_n$ with $T_i$ types).\footnote{We here omit function symbols since they are not used in this paper.}
Given a logical vocabulary $V$, a \emph{\textbf{partial} interpretation} $I$ assigns to each type symbol $T$ a finite set $I(T)$ and to each 
relation symbol $P$ with type signature $T_1\times \dots \times T_n$ a function 
\[I(P): I(T_1)\times \dots \times I(T_n)\to \{\ltrue,\lunkn,\lfalse\},\] 
where $\ltrue$ stands for true, $\lunkn$ for unknown, and $\lfalse$ for false. In case nothing is mapped to $\lunkn$, we call this an \emph{interpretation}.
A \emph{first-order theory} is a set of sentences (well-formed free-variable-free FO formulas in which each quantified variable has an associated type). 
In a second-order theory also quantifiers over relations have an associated type signature. 
A partial interpretation $I_1$ is \emph{more precise} than partial interpretation $I_2$ (notation $I_1\geqp I_2$) if $I_1$ and $I_2$ agree on everything no unknown in $I_2$.
\bart{Clear or do I make it more formal}