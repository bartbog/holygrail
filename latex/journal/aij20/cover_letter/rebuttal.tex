\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{fullpage}
\usepackage[parfill]{parskip}
\usepackage[normalem]{ulem}

\let\oldquote\quote
\let\oldendquote\endquote

\def\quoteit{\begingroup \oldquote \itshape}
\def\endquoteit{\upshape \oldendquote \endgroup}

%% Comments
\newcommand\comment[1]{\marginpar{\tiny #1}}
\renewcommand\comment[1]{#1}

\newcommand{\tias}[1]{{\comment{\color{blue}\textsc{TG:}#1}}}
\newcommand{\emilio}[1]{{\comment{Answer: \color{red}#1}}}
\newcommand{\bart}[1]{{\comment{\color{green}#1}}}
\newcommand{\new}[1]{{\comment{{\color{blue} #1}}}}
\title{Revisions: A framework for step-wise explaining how to solve constraint satisfaction problems}

\author{Bart Bogaerts, Emilio Gamba and Tias Guns}
\date{}
\begin{document}
	\maketitle
We thank the reviewers for their effort and constructive feedback. In the following, we answer the remarks of the reviewers and point out the changes we incorporated in this revision.

\section*{Reviewer \#1 - DETAILED COMMENTS}

This paper proposes a solution not only to solve a combinatorial
problem, but to explain to a human how to solve it. Therefore, the
focus is not to solve the problem as fast as possible but rather to
provide reasoning as simple as possible that leads to a solution. While
the method seems sufficiently general to work with any problem in NP,
the paper directs its efforts toward explaining how to solve grid
puzzles.

\begin{quoteit}
The paper is well written and easy to understand. I noted below a few typos that can quickly be fixed.
\end{quoteit}

\emilio{We have addressed the typos in the revised version.}

\begin{quoteit}
The literature review could be augmented with the following paper that was published before people talk about explainable AI. But in some sense, it does XAI on a grid puzzle.

Caine A., Cohen R. (2006) MITS: A Mixed-Initiative Intelligent Tutoring System for Sudoku. In: Lamontagne L., Marchand M. (eds) Advances in Artificial Intelligence. Canadian AI 2006. Lecture Notes in Computer Science, vol 4013. Springer, Berlin, Heidelberg. \url{https://doi.org/10.1007/11766247_47}
\end{quoteit}

\emilio{The proposed paper has been added to the related works section as it provides valuable insight into the modeling of the user being taught by an Intelligent Tutoring System (ITS) similar to XAI.}

\begin{quoteit}
The nested explanations can be seen as a solver branching on a variable that is forced to backtrack after detecting inconsistency. However, as solvers need to branch deep in the search tree, I could imagine having multiple levels of nested explanations. I would have liked to see how the authors manage the depth of these nested explanations. A human could easily lose track of the reasoning. How does the algorithm prevent this behavior?
\end{quoteit}

\emilio{In the case of tested logic grid puzzles, nested explanations cannot be further split into `base' explanations. For more difficult applications, this will probably not be the case. Hence, researching how to refine and abstract explanation sequences constitutes ideas for future work.}
% we zitten met stappen die we niet verder kunnen opsplitsen, voor moeilijkere applicaties zal dat niet het geval zijn en dan moeten we een manier vinden om daar slim mee om te gaan.

\begin{quoteit}
In the cost function, how did you obtain the magic number 5? How do 4 and 6 compare to 5? Some justification would be appreciated. 
I would replace 100 by M as it is done with linear programming. You can keep the definition: "The number M is taken here to be larger than any reasonable explanation size" and add "We use M=100 for our experiments".
\end{quoteit}

\emilio{
	The number 100 and 5 are indeed ``magic constants''. We have, following this suggestion, replaced them by constants M (for the large one) and N. The optimal value to assign to these constants is something we do not know yet and that requires additional research. We added a discussion in our future work section in the paragraph on \emph{`what constitutes a good explanation'}.
}

% > pseudo-randomly, kosten mogelijke avenue for future work => hinten bij introduceren van de cost in de paper dat het voor future work is.
\begin{quoteit}
The MUS solver tries to find a globally small set of constraints. Even if it does not succeed, a MUS solver aims for this goal. However, I do not think your greedy algorithm tries to achieve any globally optimal explanation.
Could that greedy algorithm reconsiders some choices in order to make better choices in the future?
\end{quoteit}

\emilio{The current greedy algorithm looks for good candidate explanations by generating constraints of increasing costs for the current state of play. It does not reconsider previous choices for better future choices. This is partly discussed in future work: \emph{`to produce easier-to-understand explanations would be optimizing the entire sequence instead of step by step..'}.}
% > explanation sequence optimaliseren + hergebruiken van SSes.

\begin{quoteit}
It is nice to see that everything was implemented in a system that reads the problem definition in natural language and outputs the explanations in natural language. However, the discussion about NLP does not bring much to the main contribution of the paper and triggers some questions.
%\end{quoteit}
%\emilio{Even though the paper covers how an NLP of the problem statement to a logic specification, the focus of the journal paper lies indeed in the explanation generation. Note that very recent work has been done on interpreting and solving logic grid puzzles using a deep neural network approach.}
%\emilio{
%	Jabrayilzade, Elgun, and Selma Tekir. "LGPSolver-Solving Logic Grid Puzzles Automatically." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 2020.}
%\begin{quoteit}
For instance, when you write: ``Since any POS-tagger can make mistakes, we make sure that all of the puzzle's entities are tagged as noun.'' How? Manually?
\end{quoteit}

\emilio{The phrasing of the puzzles are sometimes ambiguous and can lead to mistakes during the process of POS-tagging. To deal with these situations, we post-process the POS-tagged words by applying a set of pre-defined rules to recognize ambiguous cases and manually correct the assigned POS-tags.}
% Yes... :(

\begin{quoteit}
It would have been interesting to try the system on harder problems (harder than grid puzzles) to test the limits of the system.  Grid puzzles seem easy problems since you mention "we never encountered an explanation step that combined multiple clues".  I can understand that you restrict yourself to grid puzzles to better process the natural language. However, it would be interesting to test on other problems without handling the natural language.
\end{quoteit}

%\new{Hmmm... Not sure how to handle this one. }\\
\emilio{We agree on this with the reviewer. It is currently on our agenda for future work.}

Overall, this is a good paper that deserves to be published. 

Typos

Line 64: "does not have *the* all the"

Definition 6: I think you mean $I_{i-1}
$

Section 6.2: "of a derived this."



\section*{Reviewer \#2 - DETAILED COMMENTS}

This paper presents a method for step-wise explanation of solutions to
constraint satisfaction problems. The main idea is the use of a cost
function that estimates the epistemic accessibilty of the 'next'
explanation for an inference step. In this paper the authors use 'size'
for cost as a simplification, noting that average/max can be used for
sequences.  The method is agnostic to the particular solver techniques,
but the authors also show how explanation for particular applications
can be improved with new cost functions, providing a good case study in
the form of logic grid puzzles. The paper is also accompanied by a
demonstration website with two puzzles. This was very useful in
grounding the main ideas of the paper.

The interesting part of the paper is the use of a human-reasoning
'trick': reasoning by contradiction. This produces what the authors
call 'nested explanations'. The paper motivates using logic grid
puzzles. The nested explanation concept and the application are novel
as far as I know. The paper appears to be sound. CSP is outside of my
area, but the description in first-order logic is straightforward
enough to understand as the paper is very nicely written. The results
and experimentation are interesting. The key result (Figure 5) is that
nested explanations are often much less costly than their 'parent', but
sometimes they are also almost as costly -- and there are very few in
between.


Section 7: The observations in this section and how they tie to
explainining logic grid puzzles is really nicely presented and very
interesting. I imagine this would be required a bit of work and
analysis. This section is really very nice.


\begin{quoteit}
Section 5 is sorely missing an example of the more difficult explanation. THe explanation from the reasoning by contradiction is straightforward enough to understand for a lay reader, but there is no contraste to the more difficult case.
\end{quoteit}

\emilio{The clue used for the parent explanation has been highlighted. In fact, there are other nested sequences, but they tend to have a lot of nested explanations. While this case is relatively straightforward, it is a good example of how nested sequences can even improve the understanding of easier steps.}

\begin{quoteit}
It is not clear to me what happens in Algorithm 5 when f(E', S', N') $>=$ f(E, S, N). Does not explanation get stored at all for this step because it is too complicated? 
\end{quoteit}

\emilio{We have clarified this in \emph{6.4 Searching nested expanations} of the paper.}

\begin{quoteit}
The demo website is great. Thanks for taking the time to put that together. My only feedback here is that when transitivity constraints, bijectivity, etc., are added, the explanation breaks down a bit. It could be simply that I didn't really  understand what the bijectivity constraint was applied to for this particular puzzle, so changing the text here would help I guess?
\end{quoteit}

\emilio{We thank the reviewer for their suggestion and will incorporate this into the next version of the demo on the website.}

\begin{quoteit}
I very much encourage the authors to conduct some human-behavioural studies using this in future. I believe there is enough in this paper already that it is suitable, but the method (and in particular the inclusion of the observations in Section 7 for the cost function), while interesting, are not really evaluated from the perspective of whether they do what they aim to do: help people understand CSP solutions. 
\end{quoteit}

\emilio{We thank the reviewer for their suggestion and have included this idea as potential future work.}

%\emilio{We will include this suggestion as potential follow-up work.}

\begin{quoteit}
As well as looking at some studies in this space, a good place to start is Hoffman et al.,: "Metrics for explainable AI: Challenges and prospects" https://arxiv.org/pdf/1812.04608 
\end{quoteit}

\emilio{We have included this reference in the future works section.}

Very minor things:

- Page 2: "have [the] all the knowledge" -- remove the first 'the'

- Algorithm 5, line 9: has two statements on one line; maybe easier to
read if broken into two lines

- Figure 5: The x-axis has 1, then 9, then 2-8. Is this intentional? I
don't see a reason why they would be out of order all of a sudden (not
that the ordering matters, but it just seems strange)


\end{document}