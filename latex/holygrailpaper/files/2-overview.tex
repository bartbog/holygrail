%\bart{Einstein puzzle as a running example?} \tias{ if we call it holy zebra, makes sense to use Zebra as running example}

The \textbf{input} to \ourtool is a set of natural language sentences (from hereon referred to as ``clues''), and the names of the \textit{entities} that make up the puzzle, e.g. Englishman, red house, Zebra, etc. %\bart{No more mention of the fact that this is optional for the system?}

In typical logic grid puzzles, the entity names are present in the grid that is supplied with the puzzle. For some puzzles, not all entities are named or required to know in advance; a prototypical example is Einstein's Zebra puzzle, which ends with the question ``Who owns the zebra?'', while the clues do not name the Zebra entity, and the puzzle can be solved without knowledge of the fact that there is a zebra in the first place. 

%Figure \ref{fig:overview} contains an overview of the different components of the system.
%

\tikzset{%
  block/.style    = {draw, thick, rectangle, minimum height = 3em,
    minimum width = 3em,anchor=west},
  sum/.style      = {draw, circle, node distance = 2cm}, % Adder
  input/.style    = {draw, ellipse, node distance = 2cm,anchor=west}, % Input
  output/.style   = {coordinate} % Output
}


% \begin{figure*}
% \centering
% \begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
% \draw 
% %INPUT
% node [input] at (0,0) (clues){(English) clues}
% node [input] at (0,-2)  (grid) {Entities}
% %NLP
% node [block] at (4,0) (pos)  {POS tagger}
% node [block] at (4,-2)  (bos) {BOS}
% %IDP
% node [block] at (8,0) (tra) {Translator}
% node [block] at (12,0) (idp) {\idp}
% node [block] at (12,-2) (vis) {visualizer}
% ;\end{tikzpicture}
% \caption{An overview of the pipeline of \ourtool.}
% \label{fig:overview}
% \end{figure*}

\paragraph{Steps}
Our framework consists of the following steps, starting from the input:
\begin{enumerate}[A]
%  \item Natural language procesing: 
% \begin{enumerate}
\item Part-Of-Speech tagging: with each word a part-of-speech tag is associated.
\item Chunking and lexicon building: a problem-specific lexicon is developed.
\item From chunked sentences to logic: using a custom grammar and semantics a logical representation of the clues is constructed.
% \end{enumerate}
% \item Reasoning: 
% \begin{enumerate}
%   \setcounter{enumi}{3}
 \item From logic to a complete IDP specification: the logical representation is translated into the \idp language and augmented with logic-grid-specific information. \label{step:toidp}
\item Explanation-generating search in IDP: we exploit the \idp representation of the clues to search for simple explanations as to how the puzzle can be solved.
% \end{enumerate}
\item Visualisation of the explanation: the step-by-step explanation is visualized.
\end{enumerate}
% 
% 
% \item Visualisation of the explanation steps
% \end{enumerate}

The first three of these steps are related to natural language processing and are discussed in detail in the next section. Step \ref{step:toidp} is explained in Section \ref{sec:solving}; there we also briefly explain how the representation obtained at that point can be used to automatically solve the puzzle. 
The last two steps are related to explanations and are presented in Section \ref{sec:expl}. 

\section{Natural Language Processing}\label{sec:nlp}

\subsection{A. Part-Of-Speech tagging}
The standard procedure in Natural Language Processing is to start by tagging each word with its estimated Part-Of-Speech tag (POS tag). %Standard tagsets exists, as do POS taggers.

We use the standard English Penn Treebank II POS tagset \cite{DBLP:journals/coling/MarcusSM94}. As POS tagger we use NLTK's built-in Perceptron tagger\footnote{http://www.nltk.org}. It uses a statistical inference mechanism, trained on a standard training set from the Wall Street Journal. 
Since any POS-tagger can make mistakes, we make sure that all of the puzzle's entities are tagged as \textit{noun}.
%\bart{Do we check whetehr this is the case or do we enforce it somehow?}

\subsection{B. Chunking and lexicon building}
To use the Blackburn \& Bos framework \cite{Blackburn2005,Blackburn2006}, a \textit{lexicon} and a \textit{grammar} have to be provided, where the lexicon assigns a role to different sets of words, and the grammar reasons over these roles. The grammar is constructed for logic grid puzzles in general and not puzzle specific; the lexicon is partly problem agnostic and partly puzzle-specific.

We constructed a set of 12 lexical categories \cite{msc/Claes17}. The 3 puzzle-specific categories are: \textit{proper nouns}, namely the individual \textit{entities} that are central to the puzzle, \textit{other nouns} that refer to groups of entities (like house, animal) and \textit{transitive verbs} that link two entities to each other.

The other categories are general and contain a built-in list of possible members. The categories are determiner, number, preposition, auxiliary verb, copular verb, comparative and \textit{some*}-words (somewhat, sometime, ...), and conjunction. Because of space constraints, we refer the reader to the full master thesis for the full details on these categories.


The goal of this second step is hence to group the POS-tagged words of the clues into \textit{chunks} that are tagged with one of the above lexicon categories. This process is known in the NLP community as \textit{chunking}. We use NLTK and a custom set of regular expressions for chunking the proper nouns and different types of transitive verbs. %\tias{PRIVATE NOTE: I did not actually implement this (yet?), but found out that this would be the right thing to do ; )}

The result is a lexicon, which is needed as input to the subsequent step in our framework. However, the POS tagging may be inaccurate and the chunking may also miss certain cases. Furthermore, logic puzzle authors are keen to use word play or seemingly ambiguous sentences to make the puzzle more interesting, but that require general world knowledge. For example, using `in the morning' to refer to a timeslot at 11:00 when all other timeslots are after 13:00.

The automatically generated lexicon is hence tested as described in the next step, and if some clues can not be transformed into logic, the user is asked to update the lexicon or rewrite a clue. For example, to replace `in the morning' by `at 11:00' in the earlier example.


\subsection{C. From chunked sentences to logic}
The 
Blackburn and Bos framework requires a lexicon (discussed in the previous paragraph) and a grammar, each equipped with a suitable semantics.  The framework is based on the $\lambda$-calculus and Frege's compositionality principle. Every word has a $\lambda$-expression as its meaning. The meaning of a group of words is a combination of the meaning of the words that are part of the group. In this framework, $\lambda$-application is used to combine the meaning of words.

We constructed a grammar based on the first ten logic grid puzzles from Puzzle Baron's Logic Puzzles Volume 3 \cite{logigrammen}. We observe that the determiners in the logic grid puzzles we studied are simple in that only existential quantifiers are necessary. Universal quantification is not needed as the puzzles always fully classify their entities. This is a consequence of the fact that all relations described in these puzzles are bijections between the different domains. There is always exactly one person who, for example, drinks tea. For the same reason there is also no negative quantifier. Sentences like ``No person drinks tea'' do not occur in the puzzles we studied.

Since it would distract us from the core message of this paper, we do not detail the actual grammar rules here but refer the the Master thesis this paper builds on \cite{msc/Claes17} for more information. 

\ignore{
Proper nouns play an important role in our translation of logic grid puzzles into logic. We assume that every domain element (of a non-numeric) domain is represented by a proper noun (and only one). Even when the words are adjectives, they must be declared as proper nouns to the system.
Proper nouns can also be numeric, e.g. ``March''. In that case the user has to encode it into a number.

There are three words that introduce a predicate in their translation in logic. These predicates express a relation between two entities. A transitive verb links its subject and its object. A preposition (in a prepositional phrase) links the noun phrase of the prepositional phrase to the noun phrase to which it is attached. Finally, an adjective phrase as object of a copular verb, links the subject with the noun phrase that is part of the adjective phrase. This is the only place an adjective can occur in our grammar.

% An auxiliary verb can negate the verb phrase (without negating the restrictions of the subject). This is again an example of following the order of the sentence in natural language.

% A copular verb has three main uses in logic grid puzzles: with a noun phrase (in which it implicates equality), with an adjective phrase (this is the only place an adjective can occur in our grammar and it will also introduce a new predicate)

Finally there are two lexical categories which can be considered special to logic grid puzzles: comparatives and \textit{some}-words. The latter are, in the puzzles we studied, only used as an unspecified integer and always appear as a quantity in a comparison (e.g. ``somewhat more than John'').
}


Most grammar rules are quite general. They can be used outside logic grid puzzles as well. Others, like the grammar rule for a sentence with template ``Of ... and ..., one ... and the other ...'', are specific for these types of puzzles. The introduction to the logigram booklet \cite{logigrammen} explicitly mentions this template and explains how this template should be interpreted. This interpretation is incorporated in the semantics of the grammar rule covering this template.

Some other more specific rules include an \textit{alldifferent} constraint (``A, B, and C are three different persons'') and a numerical comparison (``John scored 3 points higher than Mary'').

The semantics of most rules consists only of $\lambda$-applications. Some rules are more complex. Those exceptions are either because the scope of variables and negations would otherwise be incorrect or because the grammar rule is specific to logic grid puzzles and can not be easily explained linguistically. This linguistic shortcut is then visible in the semantics.

Compared to the original Blackburn and Bos framework~\cite{Blackburn2005,Blackburn2006}, we added type information. In natural language, it is possible to construct sentences that are grammatically correct but without meaning. E.g. ``The grass is drinking the house''. The grass is not something that can drink and a house is not something that can be drunk. We say the sentence is badly typed. Based on grammar alone, we can never exclude these sentences. Thanks to adding the types, entities must not be grouped per type in advance (this is automatically detected), and verbs that are synonyms are also automatically detected.


The output of the framework is a (typed variant of) discourse representation theory \cite{DRT}.

% There are two type of noun phrases which relate two entities without a reference to this relation. Namely ``the 2008 graduate'' and ``John's dog''. In the current framework, it is impossible to derive which relation holds between ``the graduate'' and ``2008''. In Section \ref{sec:vocabulary}, we discuss a solution to this problem.

\ignore{

\todo{How does this relate to the next section? I might have screwed up the structure...}

3. (Adapted) Blackburn \& Bos
Input: [2. POS-rewriter]: the prolog fact with the rewritten sentences and the problem-specific lexicon
Static input (same for all the problems): the grammar that was derived during my thesis, the semantincs of a word-type (e.g. a verb has meaning X), the semantics of a grammar rule (how to combine the meaning of the words in case those words appear as in the grammar rule). Some of these semantics were provided by Blackburn \& Bos, some were added because they were logic puzzle specific. 
E.g. a sentence of the form "Of X and Y, one A and the other B" is defined in \url{https://github.com/bartbog/holygrail/blob/master/bos/myGrammarSemantics.pl#L33} and \url{https://github.com/bartbog/holygrail/blob/master/bos/myGrammar.pl#L91-L100} and is logic puzzle specific
Output: 
 - The First Order Logic representation of all the sentences
 - (Because it's adapted to have types): The types of all the words in the sentences

Blackburn \& Bos is a general framework with 4 inputs: lexicon, grammar, semantics for the lexicon (per word SORT, not per word!), semantics for the grammar. It works by combining the semantics of the words in the way that the semantics of the grammar specifies. It uses Discourse Representation Theory (DRT) for this because this captures quantors way better + it would work across sentences (not used in logic puzzles but this could capture the "He" in "Bart went jogging. He is tired" as "Bart"). The framework also provides a way to translate the DRT to FOL.

Because I adapted the framework, we also get a type for every word. E.g. for "Bart works in Brussels", it knows the subject of "works in" needs to be of the same type as "Bart" and the object of the same type as "Brussels". 

\paragraph{4. From logic to an IDP specification}

\todo{How does this relate to the next section? I might have screwed up the structure...}


4. Transformer to IDP:
Input: Ouputs of [3. Blackburn \& Bos], MANUAL: the range of values for the numeric types.
Output: IDP translation of all the knowledge about the puzzle including:
- Typed FOL of all the sentences in the puzzle
- relations between the different predicate in the puzzle
- Typed FOL vocabulary
- (Some problem-specific lua code, but this is just glue)

A lot of this step is just glue to translate the FOL to IDP syntax. This step also does the following however:
- It unifies the different types (across sentences). E.g. "Bart works in Brussels" and "Jens works in Leuven", it will know that Jens and Bart need to be of the same type as the subjects of "works in" need to be off the same type (similar to all other verbs). 
- It detects synonyms (based on types) and add axioms to guarantee these are respected
- It adds some rules for transitive and reflexive relations (problem-specific but based on types!)
- It adds Logigram bijection axioms (not based on types)

This step still requires the range of values for the numeric types, all other questions can be eliminated in case the problem-specific lexicon mentions which ppn's are of the same type.


\paragraph{5. Explanation-generating search in IDP}

\todo{How does this relate to the next-next section? I might have screwed up the structure...}


5. IDP Lua Code
Input: IDP Theory with the axioms + IDP Theory per sentence + Vocabulary
Output: Step-wise solution of the problem (including dependencies and sentence used to derive)

It tries to see what must hold given a theory and the axioms and a minimal set of things already known to be true/false. I hope Bart can give more info

6. Glue Code
(Not really important)
This just transforms the json from Bart a bit and saves it somewhere: \url{https://github.com/bartbog/holygrail/blob/master/bos/output/p2_types.output.json}
There is also a json from step 4 that lists all instances of all the types (grouped by type): \url{https://github.com/bartbog/holygrail/blob/master/bos/output/p2_types.voc.json}
-> It used the types for the latter but this could probably also be generated based on the input json

\paragraph{6. Visualisation of the explanation steps}

\todo{How does this relate to the next-next section? I might have screwed up the structure...}

7. Visualization
Input: The 2 json's from above
Output: A step-wise visualization
% \end{verbatim}
}
