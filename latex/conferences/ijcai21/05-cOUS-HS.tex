The first two considerations from the previous section lead to the following definition. 

\begin{definition}
   Let $\formula$ be a formula, $f:2^{\formula} \to \nat$ a cost function and  $p$ a predicate $p: 2^{\formula}\to \{\ltrue,\lfalse\}$. We call %a set $U\subseteq \formulag$ a \emph{$p$-constrained $f$-OUS} of \formulag ($(p,f)$-OUS) \tias{what with the OCUS name here?} \bart{I propsoe to say. We call 
    $\m{S} \subseteq \formula$ an OCUS of \formula (with respect to $f$ and $p$) if \begin{compactitem}                                      
      \item $\m{S}$ is unsatisfiable,
      \item $p(\m{S})$ is true
      \item all other unsatisfiable $\m{S}'\subseteq \formula$ with $p(\m{S}')=\ltrue$ satisfy $f(\m{S}')\geq f(\m{S})$.
    \end{compactitem}
\end{definition}

Rephrased in these terms, the task of the procedure \onestep is to compute an OCUS of the formula $\formula := \formulac\land I\land \overline{\Iend\setminus I}$ with $p$ the predicate that holds for subsets  that contain exactly one literal of $\overline{\Iend}$, see \cref{alg:oneStepOCUS}. 
%In the rest of this paper, we study (incremental) algorithms for computing an OCUS. 

%\emilio{why is the text formatting weird ? Due to the algorithm names ? }
In order to compute an OCUS of a given formula, we propose to build on the hitting set duality of \cref{prop:MCS-MUS-hittingset}. 
For this, we will assume to have access to a solver \cohs that can compute hitting sets of a given collection of sets that are \emph{optimal} (w.r.t.\ a given cost function $f$) among all hitting sets \emph{satisfying a condition $p$}. 
The choice of the underlying hitting set solver will thus determine which types of cost functions and constraints are possible. 
In our implementation we use a cost function $f$ as well as a condition $p$ that can easily be encoded as linear constraints, thus allowing the use of highly optimized mixed integer programming (MIP) solvers. The \cohs formulation is as follows:
%\bart{should be minimize or -f}\emilio{adapted}
\begin{align*}
\small
  minimize_S \quad & f(S) \\ 
  s.t. \quad & p(S) \\
       & sum(H) \geq 1, \quad &\forall H \in \setstohit \\
       & s \in \{0,1\}, \quad &\forall s \in S
\end{align*}
%\bart{This is not what the reviewers asked for! They asked for MIP models of our ``arbitrary objective functions''. A mip encoding of a generic hitting set problem with only some ``at least one'' constraints is not going to help us, I think. }
% well, its not arbitrary but linear, and it is a weighted sum; will have to do
%\bart{indeed, it is not arbitrary! But that's exactly the point: if we say ``we only support linear objective functions'', then that reviewers concern ``how will you encode this as MIP?'' is resolved. We simply did not say that in the prior work. In any case, the model also doesn't hurt and breaks text a bit... } 
where $S$ is a set of MIP decision variables, one for every clause in $\formula$. In our case, $p$ is expressed as $\sum_{s \in \overline{\Iend\setminus I}} s = 1$. 
%On top of that, the $p$ can be used to enforce that some constraints in \formula are hard constraints and should always be included in the hitting set. %make a distinction between hard constraints (those that \emph{must} be included in the OCUS), which can be useful in case constraints are reified using assumption literals, or to \emph{group} constraints that should be enabled/disabled simultaneously. 
%\emilio{not trivial at all!!! If I understand correctly C = Hard + indicator variables where p imposes constraints on not using the Hard, but only the indicator variables}
%\emilio{Added an example:}
%For example, when explaining logic grid puzzles, the cnf-translated hard constraints ($\mathcal{C}_{hard}$) are reified using assumption literals i.e weighted soft constraints ($\mathcal{C}_{soft}$). In that case, $\mathcal{C} = \mathcal{C}_{hard} \ \cup \ \mathcal{C}_{soft}$.
% When aiming to explain satisfaction problems in terms of the subset of constraints and literals needed to derive a new literal, the initial interpretation $I$ should consist of indicators literals for each (group of) constraint(s) as well as already known true literals. }
%\bart{Previous sentence instead of earlier ``indicator'' sentence?}
%
%In our application, 
$f$ is a weighted sum over the variables in $S$. For example, (unit) clauses representing previously derived facts can be given small weights and regular constraints can be given large weights, such that explanations are penalized for including many constraints when previously derived facts can be used instead. %relevant facts are directly available.



\newcommand\onestepo{\ensuremath{\call{explain-One-Step-ocus}}\xspace}
\begin{algorithm}[t]
  \DontPrintSemicolon
  
  \caption{$\onestepo(\formulac,f,I,\Iend)$}
  \label{alg:oneStepOCUS}
  $p \leftarrow$ exactly one of $\overline{\Iend\setminus I}$\;
  \Return{$\comus(\formulac\land I\land \overline{\Iend\setminus I}, f, p)$} 
\end{algorithm}
\begin{algorithm}[t]
  \DontPrintSemicolon
%  $\setstohit  \gets \emptyset$ \; %\label{omus-line1} 
  \While{true}{
    $\m{S} \gets \cohs(\setstohit,f,p) $  \;%\tcp*{\small Find \textb    $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \F''\}$ \;
% f{optimal} solution}
    % \tcp{\small set with all unique clauses from hitting set}
%     (sat?, $\kappa$) $\gets$ \texttt{SatSolver}($hs$)\;
    % \tcp{If SAT, $\kappa$ contains the satisfying truth assignment}
    % \tcp{IF UNSAT, $hs$ is the OUS }
    \If{ $\lnot \sat(\m{S})$}{
      \Return{$\m{S}$} \;
    }
    $\m{S} \gets  \grow(\m{S},\F) $ \label{line:grow}\;
    $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \m{S}\}$ \;
  }
  \caption{$\comus(\formula,f,p)$ }
  \label{alg:comus}
\end{algorithm}


%\tias{I would not show the above one as it is rather vague \bart{I would disagree with the vagueness. It makes abstraction of several things (what is $p$? what is $f$? How is Grow and CondOptHS implemented? But in my opinion that is good, since it shows close relation to the basic OUS algo as well as illustrating what is really going on and modularity.}, but immediately rewrite it as Alg2 the singleStepExplain:}
%\bart{I would avoid that :-) That way we mix up ``how to compute constrained OUSs?'' with ``how to compute explanations using constrained OUSs?''. These are two different concerns. We should show that we also tackle the first .  That way, our new ``singlestepexplain2'' will also look a lot simpler than singleStepExplain1 (if we use one oracle call to cOUS}

%\begin{algorithm}[ht]
%  \caption{$\call{ExplainCSPcOUS}({\cal C},f)$}
%  \label{alg:explainCSPcOUS}
%$E \gets \langle \rangle$\;
%$I_{end} \gets optimalPropagate({\cal C})$\;
%$\formulag \gets {\cal C} \cup I_{end} \cup \overline{\Iend}$\;
%$\setstohit \gets \{\formulag \setminus \{{\cal C} \cup I_{end}\}\}$\;
%// preseeding\\
%\For{$l \in I_{end}:$}{
%  $\setstohit \gets \setstohit \cup \{\formulag \setminus \grow(-l,\formulag)\} $\;
%}
%$I \gets \emptyset$\;
%$p \gets \{$exactly one of $\overline{\Iend}$ in the hitting set$\}$\; %, none of $I_{end}$ can be in the hitting set$\}$\;
%
%\While{$I \neq I_{end}$}{
%	update $p$ such that none of $\{I_{end} \setminus I\}$ and none of $\bar{I}$ can be in the hitting set\;
%    $\m{S} \gets \comus(\formulag,f,p,\setstohit)$\;
%	$I_{\mathit{best}} \gets I\cap \m{S}$\;
%    ${\cal C}_{\mathit{best}}\gets{\cal C}\cap \m{S}$\;
%	$N_{\mathit{best}} \gets \{I_{end} \setminus I\} \cap optimalPropagate(\m{S}) $\;
%	add $\{I_{\mathit{best}} \wedge {\cal C}_{\mathit{best}} \implies N_{\mathit{best}}\}$ to $E$\;
%	$I \gets I \cup N_{\mathit{best}}$\; 
%  }
%\Return{E}\;
%\end{algorithm}
Our generic algorithm for computing OCUSs is depicted in \cref{alg:comus}. It combines the hitting set-based approach for MUSs of \cite{ignatiev2015smallest} with the use of a MIP solver for (weighted) hitting sets as proposed for maximum satisfiability \cite{DBLP:conf/sat/DaviesB13}. The key novelty is the ability to add structural constraints to the hitting set solver, without impacting the duality principles of \cref{prop:MCS-MUS-hittingset} as we will show.

Ignoring \cref{line:grow} for a moment, 
the algorithm alternates calls to a hitting set solver with calls to a \sat oracle on a subset $\m{S}$ of $\formula$. 
In case the \sat oracle returns true, i.e., the subset $\m{S}$ is satisfiable, the complement of $\m{S}$ is a correction subset and is added to \setstohit. In this way, the hitting set size always grows and a hitting set $\m{S}$ will always be a subset of $\formula$.

Instead of directly adding the complement of $\m{S}$ to \setstohit as done by \citet{ignatiev2015smallest}, our algorithm includes a call to \grow, which extends $\m{S}$ into a larger subset of $\formula$ that is still satisfiable (if possible).
%\tias{here base grow impls?} \bart{Your next sentence is enough for me here. Alternatively we can in one stentence say that we consider a greedy and a maxsat-based implementation? }
We discuss the different possible implementations of \grow later and evaluate their performance in \cref{sec:experiments}.

\emilio{The result of algorithm \ref{alg:comus} represents a subset of the clauses of F if applied on a toy example with imaginary weights. Consider the following, cnf $\formula$ formula over variables $x_1, x_2, x_3$ with model $\{x_1, \lnot x_2, x_3 \}$, $p\triangleq$ \textit{exactlyoneof($c_6$, $c_7$)} and $f = \sum w_i$ the sum over clause weights $w_1 = 20, w_2=20, w_3=20, w_4=20, w_5=1, w_6=1, w_7=1$:
	\[ c_1 = x_1 \text{\hspace*{20pt}} c_2 = \lnot x_1 \vee \lnot x_2 \vee x_3\] \[ c_3 = \lnot x_1 \vee \lnot x_2 \vee x_3 \text{\hspace*{20pt}} c_4 = \lnot x_3 \vee \lnot x_2 \]
	 If we already know $I= \{x_1\}$, then remaining literals to explain are defined as $\overline{\Iend\setminus I} = c_6 \wedge c_7$:
\[c_5 = x_1 \text{\hspace*{20pt}} c_6 = x_2 \text{\hspace*{20pt}} c_7 = \lnot x_3 \]
Let $\setstohit =\{ \{ c_6, c_7\} \}$ be collection of hitting sets bootstrapped with the at least one constraint.
}

\begin{table}[!h]
	
	\begin{adjustbox}{max width=\columnwidth}
		\emilio{
	\begin{tabular}{|c|c|c|r|}
		\hline
		\rule{0pt}{2ex}$\m{S}$ & $\sat(\m{S})$ & Grow($\m{S}$, $\formula$) & $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \m{S}\}$\\ 
\hline
		\hline
		\rule{0pt}{2ex} $\{ c_6 \}$ & True & $\{ c_6,  c_1, c_5, c_2, c_3\}$    & $\{ \{ c_6, c_7\}, \{c_7, c_4\}\}$\rule{0pt}{2ex}   \\
		\hline		
		 \rule{0pt}{2ex}\multirow{2}*{ $\{ c_6, c_4 \}$}  & \multirow{2}*{True} & \multirow{2}*{$\{ c_6,  c_4, c_2, c_3\}$}   & $\{ \{ c_6, c_7\}, \{c_7, c_4\}$  \\  
		   &  &    & $\{c_7, c_1, c_5\}\}$\rule{0pt}{2ex} \\\hline
				 \rule{0pt}{2ex} \multirow{2}*{$\{ c_6, c_4, c_5 \}$}  & \multirow{2}*{True} &  \multirow{2}*{$\{ c_6,c_1, c_4, c_5, c_7 \}$}  & $\{ \{ c_6, c_7\}, \{c_7, c_4\}$  \\  
				    &  &    & $\{c_7, c_1, c_5\}, \{c_2, c_3\}\}$\rule{0pt}{2ex} \\\hline
		\rule{0pt}{2ex} $\{ c_6, c_4,c_5, c_2 \}$ & False & \multicolumn{2}{|c|}{$\{ c_6, c_4,c_5, c_2 \}$ is an OCUS of $\formula$.} \\
		\hline
	\end{tabular}
}
\end{adjustbox}
\end{table}


Soundness and completeness of the proposal follow from the fact that all sets added to \setstohit are correction subsets, and \cref{thm:soundcomplete}, which states that what is returned is indeed a solution and that a solution will be found if it exists. 
 
\begin{theorem}\label{thm:soundcomplete}
  Let $\m{H}$ be the the set of correction sets of \formula. 
  If $\m{S}$ is a hitting set of \m{H} that is $f$-optimal among the hitting sets of \m{H} satisfying a predicate $p$, and  $\m{S}$ is unsatisfiable, then $\m{S}$ is an OCUS of \formula. 
  
  If  $\m{H}$ has no hitting sets satisfying $p$, then $\formula$ has no OCUSs.
\end{theorem}
\begin{proof}
For the first claim, it is clear that $\m{S}$ is unsatisfiable and satisfies $p$. Hence all we need to show is $f$-optimality of $\m{S}$.
  If there were some other unsatisfiable subset $\m{S}'$ that satisfies $p$ with $f(\m{S}')\leq f(\m{S})$, we know that $\m{S}'$ would hit every minimal correction set of \m{F}, and hence also every set in \m{H} (since every correction set is the superset of a minimal correction set).
  Since $\m{S}$ is $f$-optimal among hitting sets of $\m{H}$ satisfying $p$ and $\m{S}'$ also hits $\m{H}$ and satisfies $p$, it must thus be that $f(\m{S})=f(\m{S}')$. 
%  

The second claim immediately follows from \cref{prop:MCS-MUS-hittingset} and the fact that an OCUS is an unsatisfiable subset of $\formula$. 
\end{proof}
% 
% 

Perhaps surprisingly, correctness of the proposed algorithm does \emph{not} depend on monotonicity properties of $f$ nor $p$. In principle, any (computable) cost function and condition on the unsatisfiable subsets can be used. In practice however, one is bound by limitations of the chosen hitting set solver. 

\emilio{Extending theorem \ref{thm:soundcomplete}, if $f$ is defined as a stricly increasing monotone function and p also satisfying monotonicity property then \comus is guaranteed to be a \emph{minimal} unsatisfiable subset.}

\begin{proposition}
	\emilio{
	The complexity of extracting the \mm{\call{O(C)US}} is of the second level of polynomial hierarchy FP$^{\sum^{P}_2}$. 
}
\end{proposition}
\begin{proof}
	\emilio{
	The extraction of a cardinality-minimal MUS (SMUS) for an unsatisfiable cnf $\formula$ is of the second level of polynomial hierarchy FP$^{\sum^{P}_2}$ \cite{ignatiev2015smallest}. In the case of $f$ being a weighted sum, the \omus extraction is reducible to an SMUS-extraction on a relaxed formula $\formula^{R}$ in polynomial time. 

		\noindent\textit{Reduction.} Associate for each clause $c_i \in \formula$ of weight $w_i=n \in \mathbb{N}^+$, the relaxation variables $r_{i,j}$ where $j \in 1..n-1$:
		\[ \formula^{R}  \triangleq \bigcup_{c_i \in \formula}(c_i \leftrightarrow (r_{i, 1} \wedge ..  r_{i, n-1})) \]

%	$F^{R} \triangleq \cup \$
%	, by adding $w_i - 1$ relaxation variables $(r_{i,1} \wedge .. \wedge r_{i,n-1}) \leftrightarrow c_i$, the problem of OUS is reduced to the problem of SMUS.
	}
	\paragraph{Constrainedness.} \todo{Add proof for Constrainedness}
\end{proof}
% Now, since the search for optimal hitting sets is --- in implicit hitting set algorithms --- usually done with a MIP solver, it suffices to express the predicate $p$ as constraints on the MIP. Since the variables of the MIP encoding represent inclusion of certain constraints in the unsatisfiable subset, this is simple for  the 3 constraints that we need to obtain meaningful explanations. %needed to have meaningful in practice only predicates $p$ that can easily be encoded in MIP are useful. In such cases, we can directly use the MIP solver to implement \cohs as well. 

% \paragraph{Application to Explanations}
% %To apply this idea to the context of explanations, we note that at each step, the current interpretation, will be fixed. 
% %At each step, we are looking for an OUS that contains \emph{exactly one} negation of a derivable literal. 
% %Such an exactly-one constraint is easily expressible in MIP.
% %Furthermore, also the ``subtheory constraint'', as introduced for incremental MUS solving can be expressed in MIP. Namely, in \cref{sec:incremental}, we assumed that each OUS call would be done given a subtheory of the original theory. However, constraints of the form ``the OUS should be a subset of the given set \formula'' are easily expressible in MIP as well. 
% %As such, the idea of constrained OUS computation is actually more general than the formalization of incremental OUS. 
% % 
% Given such a constrained OUS algorithm, the procedure to find the single best explanation step now simplifies to \cref{alg:singleStepExplain3}.
% 
% \begin{algorithm}[t]
%   \caption{$\call{bestStep--c-OUS}({\cal C},f,I,I_{end})$}
%   \label{alg:singleStepExplain3}
% $\formulag \gets {\cal C} \cup I_{end} \cup \overline{\Iend}$\;
% set $p$ such that exactly one of $\overline{\Iend}$ in the hitting set \textit{and} none of $\{I_{end} \setminus I\}$ \textit{and} none of $\bar{I}$ can be in the hitting set\;
% \Return{$\comus(\formulag,f,p)$}\;
% \end{algorithm}

% \tias{hard/soft temporarily hidden}
% \ignore{

