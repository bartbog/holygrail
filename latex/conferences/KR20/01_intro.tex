% !TeX root = ./workshop_paper.tex
In the last few years, as AI systems employ more advanced reasoning mechanisms and computation power, it becomes increasingly difficult to understand why certain decisions are made.
Explainable (XAI), a subfield of AI, aims to fulfil the need for trustworthy AI systems to understand why and how the system made a decision, for verifying correctness of the system, as well as to control for biased or systematically unfair decisions.

Explanations have been investigated in constraint solving before, most notably for explaining overconstrained, and hence unsatisfiable, problems to a user.
The QuickXplain method \cite{junker2001quickxplain} for example uses a dichotomic approach that recursively partitions the constraints to find a minimal conflict set. Many other papers consider the same goal and search for explanations of over-constrainedness~\cite{leo2017debugging,zeighami2018towards}.
A minimal set of conflicting constraints is often called a \emph{minimal unsatisfiable subset} (MUS) or \emph{minimal unsatisfiable core} \cite{marques2010minimal}. Despite the fact that we do not (specifically) aim to explain overconstrained problems, our algorithms will internally also make use of MUS extraction methods.

While explainability of constraint optimisation has received little attention so far, in the related field of \textit{planning}, there is the emerging subfield of \textit{eXplainable AI planning} (XAIP)~\cite{fox2017explainable}, which is concerned with building planning systems that can explain their own behaviour.
This includes answering queries such as ``why did the system (not) make a certain decision?'', ``why is this the best decision?'', etc. In contrast to explainable machine learning research~\cite{guidotti2018survey}, in explainable planning one can make use of the explicit \textit{model-based representation} over which the reasoning happens. 
Likewise, we will make use of the constraint specification available to constraint solvers, more specifically typed first-order logic~\cite{atcl/Wittocx13}.

In order to provide Explainable Agency to the constraint solver, we first formalize the problem of step-wise explaining the propagation of a constraint solver through a sequence of small inference steps.
Next, we introduce an algorithm that uses an optimistic estimate of a given cost function quantifying human interpretability to guide the search to simple, low-cost, explanations thereby making use of Minimal Unsatisfiable Subsets.
Our algorithm provides explanations for inference steps involving arbitrary combinations of constraints and produces additional explanations of complex inference steps using reasoning by contradiction.
