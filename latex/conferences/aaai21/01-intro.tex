% !TeX root = ./main.tex
\emilio{Rephrase Intro: In the last few years, as AI systems employ more advanced reasoning mechanisms and computation power, it becomes increasingly difficult to understand why certain decisions are made.
Explainable (XAI), a subfield of AI, aims to fulfil the need for trustworthy AI systems to understand \emph{how} and \emph{why} the system made a decision, e.g. for verifying correctness of the system, as well as to control for biased or systematically unfair decisions.

Despite the fact that we do not (specifically) aim to explain over-constrained problems, our algorithms will also internally make use of methods to extract a minimal set of conflicting constraints often called a \emph{\underline{M}inimal \underline{U}nsatisfiable \underline{S}ubset} (MUS) or \emph{Minimal Unsatisfiable Core} \cite{marques2010minimal}.

While explainability of constraint optimisation has received little attention so far, in the related field of \textit{planning}, there is the emerging subfield of \textit{eXplainable AI planning} (XAIP)~\cite{fox2017explainable}, which is concerned with building planning systems that can explain their own behaviour.
This includes answering queries such as ``why did the system (not) make a certain decision?'', ``why is this the best decision?'', etc. In contrast to explainable machine learning research~\cite{guidotti2018survey}, in explainable planning one can make use of the explicit \textit{model-based representation} over which the reasoning happens.
Likewise, we will make use of the constraint specification available to constraint solvers, more specifically typed first-order logic~\cite{atcl/Wittocx13}.

This research fits within the general topic of Explainable Agency~\cite{langley2017explainable}, whereby in order for people to trust autonomous agents, the latter must be able to \textit{explain their decisions} and the \textit{reasoning} that produced their choices.
To provide the constraint solver with Explainable Agency~\cite{langley2017explainable}, we first formalize the problem of step-wise explaining the propagation of a constraint solver through a sequence of small inference steps.
Next, we use an optimistic estimate of a given cost function quantifying human interpretability to guide the search to \textit{simple}, low-cost, explanations thereby making use of minimal unsatisfiable subsets.
We extend this approach using \emph{reasoning by contradiction} to produce additional explanations of still difficult-to-understand inference steps.
Finally, we discuss the challenges and some outlooks to explaining how to solve constraint satisfaction problems.


\paragraph*{Publication history} This workshop paper is an extended abstract of previous papers presented at workshops and conferences \cite{claesuser,DBLP:conf/bnaic/ClaesBCGG19,ecai/BogaertsGCG20} and a journal paper under review \cite{bogaerts2020framework}.
}

\begin{enumerate}
    \item XAI
    \item MUS vs Overconstrained/Infeasibility
    \item CSP
    \item XOPT
\end{enumerate}

Contributions : 
\begin{enumerate}
    \item Efficient, greedy algorithm for explaining CSP based on OMUS 
    \item Adapatation smallest MUS adaptation to OMUS (different hs + no maxsat) 
    \item Improving OMUS algorithm from Davies' related delayed MaxSAT algorithm 
    \item Incremental OMUS computation to speed-up explanations
    \begin{itemize}
        \item hoe belangrijk is de incrementaliteit in het algorithme
        \item kunnen  we nog meer incrementeel verder gaan
    \end{itemize}
    \todo{
    \begin{itemize}
        \item Bestaande SMUS/OMUS algorihtmes incrementeel veranderen
        \begin{itemize}
            \item Bredere studie, SMUS => OMUS veralgemenen
            \item Hoe efficient zijn ze om ons probleem op te lossen
        \end{itemize}
    \end{itemize}
    }
\end{enumerate}

