Algorithm~\ref{alg:comus} is generic and can also be used to find (unconstrained) \omus{}s, namely with a trivially true $p$.
However, its constrainedness property allows to remove the need to compute a MUS/\omus for every literal. This decreases the complexity of explanation sequence generation from $O(n^2)$ calls to MUS to $O(n)$ calls to OCUS, namely, once for every step in the sequence. 

We now discuss optimizations to the OCUS algorithm that are specific to explanation sequence generation, though they can also be used when other forms of domain knowledge are present. 
%In this section, we discuss optimizations to the basic algorithm to compute OCUSs. 
%While our optimizations are inspired by the explanation generation problem, and tailored to solving this as efficiently as possible, the ideas presented here can also 
 
\paragraph{Incremental OCUS computation}
%While searching for unsatisfiable subsets satisfying structural constraints already greatly reduces the number of calls to the unsatisfiable subset oracle, i
Inherently, generating a sequence of explanations still requires as many OCUS calls as there are literals to explain. 
Indeed, a greedy sequence construction algorithm %high-level algorithm, not explicitly portrayed in the current paper, 
calls \onestepo iteratively with a growing interpretation $I$ until $I=\Iend$.

All of these calls to \onestepo, and hence OCUS, are done with very similar input (the set of constraints does not change, and the $I$ slowly grows between two calls). For this reason, it makes sense that information computed during one of the earlier stages can be useful in later stages as well. 

The main question is, suppose two \comus calls are done, first with inputs $\formula_1$, $f_1$, and $p_1$, and later with $\formula_2$, $f_2$, and $p_2$; how can we make use as much as possible of the data computations of the first call to speed-up the second call? The answer is surprisingly elegant. The most important data \comus keeps track of  is the collection \setstohit of correction subsets that need to be hit.

This collection in itself is not useful for transfer between two calls, since -- unless we assume that $\formula_2$ is a subset of $\formula_1$, there is no reason to assume that a set in $\setstohit_1$ should also be hit in the second call. 
However, each set $H$ in $\setstohit$ is the complement (with respect to the formula at hand) of a \emph{satisfiable subset} of constraints, and this satisfiability remains true. 
Thus, instead of storing $\setstohit$, we can keep track of a set \satsets of \emph{satisfiable subsets} (the sets $\m{S}$ in the \comus algorithm). 
When a second call to \comus is performed, we can then initialize $\setstohit$ as the complement of each of these satisfiable subsets with respect to $\formula_2$, i.e., \[\setstohit\gets \{\formula_2\setminus \m{S}\mid \m{S}\in \satsets\}.\]

The effect of this is that we \textit{bootstrap} the hitting set solver with an initial set $\setstohit$.

For hitting set solvers that natively implement incrementality, we can generalize this idea further: we know that all calls to $\comus(\formula,f,p)$ will be cast with $\formula \subseteq \m{C}\cup \Iend \cup \overline{\Iend \setminus I_0}$, where $I_0$ is the start interpretation. Since our implementation uses a MIP solver for computing hitting sets (see Section~\ref{sec:backgr}), and we have this upper bound on the set of formulas to be used, we can initialize all relevant decision variables once. To compute the conditional hitting set for a specific $\formulac\cup I\cup \overline{\Iend\setminus I} \subseteq \m{C}\cup \Iend \cup \overline{\Iend \setminus I_0}$ we need to ensure that the MIP solver only uses literals in $\formulac\cup I\cup \overline{\Iend\setminus I}$, for example by giving all other literals infinite weight in the cost function. In this way, the MIP solver will automatically maintain and reuse previously found sets-to-hit in each of its computations. 
% \emilio{need to re-read reviews, reviewers asked (?) for simple MIP model of the problem}
%\emilio{Maybe an introductory text?}
%\paragraph{Bounded OUS computation for efficient explanation-generation}
%1. Leg eerst uit he bounded OUS kan werken (gegeven een bound, stop als je HS hogere waarde heeft)
%2. Leg dan uit hoe je bounded OUS kan gebruiken in de lus
%3. Leg dan de optimalisatie met sorteren uit en waarom we dat doen (zie comments)
\paragraph{Bounded OUS computation} The principle of reusing information from previous MUS/\omus calls can also be used when computing an \omus{} for every literal separately, more precisely in Algorithm~\ref{alg:oneStep} where MUS can be replaced by \omus. 
\bart{What does replacing MUS by \omus have to do with reusing information? Nothing.}
Furthermore \bart{This ``furthermore'' is the *only* ``reusing information'' we would do. Furthermore is not the right word for that}, we can push the check of line \ref{alg:oneStep:ifcheck} into the \omus call either by passing it directly to the hitting set solver if it accepts an upper bound $ub$ (like MIP solvers do); or by adding a condition on the found $\m{S}$ before the \sat call %at line \ref{alg:ocus-sat-check} of
Algorithm~\ref{alg:comus} \bart{to explain how OCUS corresponds to OUS}, i.e interrupt this \omus-call if $f(\m{S}) > ub$.  \bart{This is very technical. This is \emph{how} we do something. What is missing is \emph{what} we want, namely a bounded OUS check.}
Since the bounding on the \omus cost has the most effect if cheap \omus{}s are found early in the loop, we sort the literals based on their corresponding upper bound \bart{Which is? Where does it come from?} , initialized at $f(\formulac \land I_0 \land \overline{\Iend \setminus I_0})$.

\paragraph{Explanations with Bounded OUS}
%Context: how to do OUS 
Instead of working OCUS-based, we can now also generate optimal explanations by replacing the MUS call by an OUS call  in Algorithm~\ref{alg:oneStep} (where OUS is computed as in Algorithm~\ref{alg:comus}, but with a trivially true $p$). 
%Observation: waste of time
When doing this, we know that every OCUS of cost greater than or equal to $f(X_{\mathit{best}})$ will be discarded by the check on Line 4 of Algorithm~\ref{alg:oneStep}.
%Optimizatino: what do we do? 
As such, a next optimization is to, instead of searching for an OUS, perform a \emph{bounded OUS check}, which only computes an OUS in case one of cost smaller than a given bound $\mathit{ub}$ exists.  
%Detail: how do we do it? 
In our specific implementation, bounded OUS is performed by interrupt this \omus-call (after Line 3 Algorithm~\ref{alg:comus}) if $f(\m{S}) > \mathit{ub}$.
Since the bounding on the \omus cost has the most effect if cheap \omus{}s are found early in the loop, we keep track of an upper bound of the cost of an OUS for each literal to explain. This is initialized as a value greater than any OUS, e.g., as $f(\formulac \land I_0 \land \overline{\Iend \setminus I_0})$, and is updated every time an OUS explaining that literal is found.


%\emilio{At every explanation step, whenever an \omus is found, it provides a tighter upper bound on the cost of the best candidate explanation. 
%	In subsequent calls to \omus, we can interrupt the \omus computation if the cost of the computed hitting set exceeds the upper bound.
%	More generally, we keep track of the upper bound for every remaining literal to explain. In order to potentially guide the search for the next best explanation, we sort the remaining literals-to-explain according to their best upper bound found so far.}
%% Macros kapot, files weer verplaatst. Ik ga het niet oplossen. Ge kunt mijn commetns lezen.
%\bart{This is NOT about OUS computation. This is about OUS-based explanations. }
%\bart{Clarify that the first part of the text is about within an entire n2 loop. That the ``subsequent calls to ous are in the same loop''. } 
%\bart{``The upper bound for every remain literal'' does not exist. Het kan zijn dat bepaalde literals nog geen upper bound gekregen hebben}
%\bart{In order to potentially guide -> Te vaag en vrijblijvend. Eerder iets als ``Since our optimization ... has the most effect if cheap OUSs are found early in the loop, we sort ....''  }
%\bart{Ook het woord  \emph{bounded} moet hierin komen. Paragraaf titel wordt iets als \emph{Bounded OUS computation for efficient explanation-generation} }


%\tias{Would put 'Bounded OUS computation for explanation-generation' paragraph here, e.g.: The principle of reusing information from previous MUS/OUS calls can also be used when computing an OUS for every literal separately. When considering Algo 1, we can replace MUS by OUS. Furthermore, we can push the check of line 4 into the OUS call if the hitting set solver accepts an upper bound like MIP solvers do. In this case... furthermore, sorting line 2...}
% \paragraph{Using Initialization to Add Domain Knowledge} ? 
% \todo{TO BE DETERMIEND IF WE WANT TO SAY SOMETHING AOBUT PRESEEDING.}\bart{ I still dont believe it makes a difference. TO be tested first!} {\color{red}The current experiments confirm (finally) that this is useless. So let us just drop it.  Honestly, this gives me more trust in the experiments :-)   }



\paragraph{Domain-Specific Implementations of \grow} \label{para:domainspecificgrow}
%In the context of explanations we are in, we have a way of generating large satisfiable subsets quite easily and hence we conjecture that implementing a domain-specific \grow will be more efficient than generic version of the \grow algorithm. 

The goal of the \grow procedure is to turn $\m{S}$ into a larger subformula of $\formula$. The effect of this is that the complement added to \setstohit will be smaller, and hence, a stronger restriction for the hitting set solver is found.  

Choosing an effective \grow procedure requires finding a difficult balance: on the one hand, we want our subformula to be as large as possible (which ultimately would correspond to computing the maximum satisfiable subformula), %to derive the most precise information we can get, 
but on the other hand we also want the procedure to be very efficient as it is called in every iteration. 
%<<<<<<< HEAD
%%\deleted{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$, however, by doing that we expect the \grow costs to be too high to make the overall OCUS algorithm faster.}
%Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$. However, as will become clear from the experiments, the cost of such a \grow is too high to make the overall OCUS algorithm faster. 
%=======
%\deleted{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$, however, by doing that we expect the \grow costs to be too high to make the overall OCUS algorithm faster.}
%\bart{This is extremely confusing. You seem to suggest here that we do NOT want to do maxsat. But we do!Ã¨ I would not include this here. Do we have evidence that a comlete maxsat would be better (if we ignore the cost of the grow? I don't know if it is} 
%\emilio{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$. However, as will become clear from the experiments, the cost of such a \grow is too high to make the overall OCUS algorithm faster. }
%>>>>>>> 9bb2659817f6b9551a7557006e9a509a509d6f19

For the case of explanations we are in, we make the following observations: 
\begin{itemize}
 \item Our formula at hand (using the notation from the \onestepo algorithm) consists of three types of clauses: \begin{inparaenum}\item  (translations of) the problem constraints (this is \formulac) \item literals representing the assignment found thus far (this is $I$), and \item the negations of literals not-yet-derived (this is $\overline{\Iend\setminus I}$). \end{inparaenum}
 \item $\formulac$ and $I$ together are satisfiable, with assignment $I_{end}$, and \emph{mutually supportive}, by this we mean that making more constraints in \formulac true, more literals in $I$ will automatically become true and vice versa. 
 \item The constraint $p$ enforces that each hitting set will contain \textbf{exactly} one literal of  $\overline{\Iend\setminus I}$
\end{itemize}
Since the restriction on the third type of elements of $\formula$ are already strong, it makes sense to use the \grow(\m{S},\F) procedure to search for a \emph{maximal} satisfiable subset of $\formulac\cup I$ with hard constraints that $\m{S}$ should be satisfied, using a call to an efficient  (partial) \maxsat solver. Furthermore, we can initialize this call as well as any call to a \sat solver with the polarities for all variables set to the value they take in $\Iend$. %This is likely to lead to a fairly large satisfiable subset and hence a good correction set. 

We evaluate different grow strategies in the experiments section, including the use of partial \maxsat as well as weighted partial \maxsat based on the weights in the cost function $f$.

\paragraph{Example 1 (cont.)} Consider line 0 in table \ref{tab:explanation-steps-expanded}. During the \grow procedure, the \maxsat solver \emph{Max-Actual-Unif} with polarities set to \Iend branches when multiple assignment to a literal are possible. By hinting the polarities of the literals, we guide the solver and it assigns all values according to the end interpretation and neither $c_6$ nor $c_7$ is taken.




 
% \todo{``old maxsat'' comes here}



