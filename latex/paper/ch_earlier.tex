\section{Related work}

Two of the most advanced controlled natural languages that can be used for knowledge representation are Attempto Controlled English (ACE) and Processable English (PENG). ACE \cite{Fuchs2008} is a general purpose CNL. The language contains a large built-in vocabularium. This has as advantage that the user doesn't have to provide the vocabularium. However, for specifications in small domains, we usually do want to provide the vocabularium to make sure the specification only talks about the modelled domain. PENG \cite{Schwitter2002} does ask the user to provide its own content words. Moreover, there is a tool named ECOLE which gives suggestions while writing PENG sentences, namely which linguistic constructs can follow. This makes it easier to write correct PENG sentences. These languages mainly show that we can translate English into logic, but the literature on both these CNL's lacks a description of this translation process. Therefore, extending these CNL's is hard.

Another important CNL is RuleCNL \cite{Njonko2014}. It is a CNL that translates (a subset of) English into business rules. It's notable because it is the only CNL we could find that uses types. Interestingly, they don't really describe their type system nor the inferences they support. However figure~6 from \cite{Njonko2014} indicates that they do support type checking of business rules in RuleCNL.

Finally, Baral et al. \cite{Baral2008, Costantini2010, Baral2012, Baral2012a} researched translating natural language into ASP programs. They do this based on $\lambda$-calculus and a Combinatorial Categorial Grammar (CCG). Each word gets one $\lambda$-ASP-expression per CCG category it can represent. The CCG then tells how to combine these expressions via $\lambda$-application. In their last paper \cite{Baral2012a}, Baral et al. explain how to learn both these $\lambda$-ASP-expressions and a probabilistic CCG grammar from a set of logigrams. They used a supervised machine learning method for this goal. Based on these methods they can solve unseen logigrams without adapting these logigrams.

However, the goal of this paper is not to solve (unseen) logigrams without adapting them. The goal is to create a CNL with formal semantics that can be applied to logigrams and can be used as a knowledge representation language for these logigrams. This paper illustrates that for a small domain, it is possible to have such a language. For unseen logigrams, slight adaptions are necessary to make sure the clues are grammatically correct according to the constructed CNL. As mentioned earlier, once we have a representation of the clues in a formal logic, inference becomes possible. Solving the logigram is only one of the examples.
