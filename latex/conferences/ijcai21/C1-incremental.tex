% \newcommand\satsets{\mm{\mathbf{SSs}}}
% \newcommand\fall{\mm{\formula_{\mathit{all}}}}
In this section, we are concerned with a setting in which we know that many related (but not identical) OUS calls will occur. The goal is to extend the basic OUS algorithm to exploit this knowledge as much as possible, preferably without too much overhead for the individual calls. 
% 
To formalize this, we assume several \omus calls will happen with different formulas $\formula_1,\dots,\formula_n$ with a non-empty intersection.\footnote{In fact, the algorithm we present also works for completely unrelated formulas but will in this case not yield any benefit over the plain \omus algorithm of \cref{sec:omus}.}

The most important data the plain \omus algorithm keeps track of is a set \m{H} of correction subsets of \formula.
While these correction subsets themselves are not necessarily correction subsets of other related formulas, their complements do transfer between formulas.

\begin{proposition}\label{prop:ss}
if $\m{S} \subseteq \formula_1$ is a satisfiable subset of $\formula_1$, then $\m{S}\cap \formula_2$ is a satisfiable subset of $\formula_2$.
% \bart{Not so convincing to call this a proposition (rather than ``trivial observation''}
\end{proposition}
Hence, $\formula_2 \setminus \{\m{S}\cap \formula_2\}$ is a correction subset of $\formula_2$ and can be added to the sets-to-hit $\setstohit$. As before, we can first grow the satisfiable subset $\grow(\m{S}\cap \formula_2,\F_2)$ to obtain a larger satisfiable subset and hence a smaller correction set.

We use this property to create an \textit{incremental} OUS algorithm, see \cref{alg:omus-inc}. We assume that our algorithm has access to a set $\satsets$ in which we store satisfiable sets used to build $\setstohit$. The \omus algorithm is then initialized with the corresponding correction subsets. 
In  \cref{alg:omus-inc}, this is done in the for-loop starting at \cref{line:for-inc-seed}. The if-test in that loop is an optimization that does not affect correctness but does ensure that we will not compute the same satisfiable subset over and over again, to avoid spurious calls to \grow. 
Otherwise, the algorithm behaves identically to \cref{alg:omus}, except that whenever a satisfiable subset is found, it is stored for use in later calls to \omusinc (\cref{line:store}). 
% \tias{I find $SSs_F$ and $SS$s to be too similar and a possible cause of confusion; not sure how to rename the former... Bart?}\bart{better?}


% the existence of some unsatisfiable formula $\fall$ and that several calls to compute an OUS of a \emph{subset} of \fall will happen. 
% To take this into account in our OUS algorithm, we first note that satisfiable subsets of $\formula\subseteq \fall$ are also satsifiable subsets of \fall, and hence their complement (in \fall) are correction sets.
% Now, since the core OUS algorithm is to store correction sets, we propose that these be stored over multiple calls, resulting in the following algorithm, which we assume has access to a shared data structure containing \fall and a set \setstohitall of correction subsets of \fall. 
% \newcommand\Fall\fall
% \newcommand\Hall\setstohitall
% \tias{hmm hmm... I was not thinking of doing THIS already here...
% I think we should merely explain that if we store the satisfiable subses S used to generate the sets-to-hit (F-S), and we need to do a related OMUS, we can 'seed' the sets-to-hit by doing F-grow(S intersection F); because S intersection F will still be a satisfiable subset of F, and hence we can grow it to F, and hence we can take the complement to get an MCS, and hence that can already be put in H instead of restarting from scratch each time... I guess that is the first half of the algo and what you explain. Note that we must store the MSS'es, not the MCSs $h$ as you currently do in the algo}
% \bart{
% \begin{itemize}
%     \item My way of phrasing the algorithm does not do a grow during the seeding. Instead, it does an extra grow on line 12. Growing to a subset of Fall instead of F. This means you only have to do this grow once, instead of with every subsequent call. 
%     \item The current presentation followed from discussion we had earlier. THere, we noticed that if you store the MSS of F, not Fall, and then take intersection with another F', there is a big chance that the intersection just falls in the ``trivially satisfiable part'' (the puzzle) and thus that the growing yields something suboptimal. (the same over and over again). The current presentation avoids this. To see if it is close enough to the actual implementation :-) 
%   \item Storing MSS vs MCS should not matter. We know Fall anyway so we can easily go back and forth. I opted here for storing the MCSs for consistency of presentation (that is also what standard OMUS stores) 
%   \item As for presentation, 
% \end{itemize}
% 
% }\tias{That is also what we implement and what we experimentally compare to the other methods...}



\newcommand\satsetsformula{\mm{\mathbf{SSOfF}}}
% \newcommand\setstohit{\ensuremath{\m{H} }\xspace}
\begin{algorithm}[ht]
  \DontPrintSemicolon
  %$\setstohit  \gets \emptyset$ \; %\label{omus-line1} 
  $\satsetsformula\gets\emptyset$\;
  \For{$S\in\satsets$ \label{line:for-inc-seed}}{
    $S_\formula \gets {S}\cap \formula$\;
    \If{$\lnot \exists {S}'\in\satsetsformula: {S}_\formula \subseteq{S}'$}{
    	$S_\formula \gets \grow( {S}_\formula, \formula)$\;
    	        $\setstohit \gets \setstohit\cup \{\formula \setminus {S}_\formula\}$\;
        $\satsetsformula\gets\satsetsformula\cup\{{S}_\formula\}$
        }
    }
  \While{true}{
    $\F' \gets \ohs(\setstohit,f) $ \label{smus-hs} \;
    \If{ $\lnot \sat(\F')$}{
      \Return{$\F'$} \;
    }
    $\F'' \gets  \grow(\F',\F) $\;
    $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \F''\}$\;
    $\satsets\gets \satsets\cup \{(\F'',M)\}$ \label{line:store}
    \;
  }  \caption{$\omusinc(\formula,f)$ }
  \label{alg:omus-inc}
\end{algorithm}
% 
% \tias{Also, then I would add pre-seeding: as we know our OMUSs will be variants of I and G with an l; we can grow(l) for each l, which will be a satisfiable subset and hence can be used to seed the different incremental OMUS calls. Also experimentally this is what is being compared in the mix}\bart{OK, but add it here? This is *another* way of adding domain knowledge, right? We know from teh structure of the problem that certain sets will be satisfiable. We can use it to inform our algorithM. Very similar to the domain knowledge added in the next section.}
% 
% The differences with \cref{alg:omus} can be explained as follows.
% First of all, instead of initializing \setstohit as the empty set, we store all intersections of sets in \setstohitall with the current formula in it. 
% While this could simply be done using 
% % \[\setstohit \gets \{h\cap \fall\mid h\in \Hall\},\]
% we instead make use of a procedure \store, whose function is to remove possible duplicates and possible non-subset-minimal correction sets. 
% Indeed, it might be the case that for $h_1,h_2\in TODO$ %\setstohitall$,
% $h_1\cap\formula \subsetneq h_2\cap\formula$. 
% While it would not influence correctness of the algorithm, keeping both would influence space and time consumption.
% Another difference is that after computing a satisfiable subset $\formula''$ of \formula, in \cref{lin:storegrow} we grow it once more to obtain a (possibly larger) satisfiable subset of FALLTODO, which is then subsequently stored in HALLTODO. Again, the possibility extends that this is non subset-minimal and the \store procedure takes care of that.

\paragraph{Using Initialization to Add Domain Knowledge}
Before the first call to \omusinc, the set \satsets of satisfiable subsets will be empty. 
However, in case we have structural domain knowledge about the formulas, this can be exploited to pre-populate this set intelligently. 
For instance, in the explanatory setting, we know that $\formulac$ is satisfiable with (partial) solution $\Iend$, and \formulac is a satisfiable subset for \emph{all} \omusinc calls to follow. 
By initializing $\satsets$ as $\{\formulac \cup I_{\mathit{end}}\}$, we avoid the algorithm having to rediscover this satisfiable subset itself.
%, and hence can potentially result in serious performance gains. This is experimentally validated in \cref{sec:experiments}.

Furthermore, we know that for every literal $ l \in \Iend$ we will do an OMUS call. Hence, we can pre-compute a large satisfiable subset for every $\lnot l$ through $\grow(\lnot l, \formulag)$, with $\formulag = {\cal C} \cup \Iend \cup \overline{\Iend}$ the set of all clauses that will ever be considered in the OUS calls. We call this process \textit{pre-seeding} the set $\setstohit$.

Apart from optionally pre-seeding and calling $\omusinc(\formula,f)$ instead of $\omus(\formula,f)$ on line 3 in Algorithm~\ref{alg:bestStepOUS}, there is one more property we can use: the OUS found will only be kept if $f(X)<f(X_\mathit{best})$.
Furthermore, the hitting sets found will only increase in cost. As such, as soon as a hitting set is found with a cost exceeding the best found cost yet, we know that we can exit that \omus call since its result will not be used anymore. 
% On top of that, 
% 
% 
% Instead of enforcing this post-hoc, we can push this constraint into the OUS algorithm through a simple upper-bounding constraint added to the hitting set algorithm in \omusinc. 
Furthermore, as it is obviously beneficial to find a good $X_\mathit{best}$ quickly, we can order the literals by their best value found in the previous iteration, and at the first step, by the  value found while pre-seeding. 
% \bart{This ``upper-bounding constraint''. Is actually a form of ``Constrained OUS'' and hence belongs more in the next section, or maybe in implementation considerations}

%\tias{seeding}
%\begin{algorithm}[ht]
%$\formulag \gets \formulac \cup \Iend \cup \overline{\Iend}$\;
%$\setstohit \gets \{\formulag \setminus \{\formulac \cup \Iend\}\}$\;
%\For{$l \in \Iend:$}{
%  $\setstohit \gets \setstohit \cup \{\formulag \setminus \grow(-l,\formulag)\} $\;
%}
%\end{algorithm}


%\paragraph{Application to Explanations}
% \todo{TO BE REWRITTEN}
% To apply this idea to the context of explanation generation, we notice that the entire explanation generation loop starts with a certain interpretation $I_0$, and gradually derives more and more consequences.
% At each point, the set of literals that can be used for explanations are the consequences derived this far. On top of that, the \omus call will always contain a single negation of a consequence literal (the literal to be explained). 
% Thus, if 
% \[I_{\mathit{end}} = \{ l\mid \formulag \land I_0 \models l\},\]
% then we can take 
% \[\fall = \formulag \cup I_{\mathit{end}} \cup \lnot I_{\mathit{end}}.\]
% Which is clearly unsatisfiable (containing various literals and their negation). However, the \omus calls will always be on a subset of \fall that contains no such obvious consequences. 
%
%Using \omusinc instead of \omus in \cref{alg:singleStepExplain}  then results in an incremental explanation algorithm. Also note that the incrementality is not just obtained in a single step but over the different explanation steps. 
% 
% assu
% 
% 
% 
% We are now ready to present our basic OMUS algorithm in \cref{alg:omus}. 
% The algorithm keeps track of a set $\m{H}$ of (minimal) correction subsets of $\formula$. 
% It makes use of a procedure \ohs that computes an optimal (with respect to $f$) hitting set $\formula'$  of a given set of subsets of \formula. In practice, this type of call is often implement using a MIP solver \cite{davies2011solving}. 
% Whenever such a hitting set is found, a \sat-call checks whether the result is satisfiable. If it is, \cref{prop:K} guarantees that the result is an OMUS. 
% If it is not, a procedure \grow is used to extend it to a set $\formula''$ with $\formula'\subseteq \formula''$ such that $\formula''$ is still satisfiable.
% The implementation of \grow can be achieved in different ways.
% In fact, we could call a weighted partial \textsc{MaxSAT} solver to find the maximal satisfiable subset of clauses grown from the hitting set.
% In practice, we use a greedy approximative method to find a sastisfying assignment favoring literals that will satisfy the most clauses of highest weights.

% An attempt at describing the generic, incremental, setting.
% 
%  
% 
% We have a theory (a set of constraints) T plus a weight functionn assigning weights to elements of it (or some more complex way to compute costs of subsets of T) 
% 
%  
% 
% 
% There will be a lot of OMUS calls, but not for OMUSs of T, but for OMUSs of subsets of T
% 
%  
% 
% I.e.,  
% OMUS(T') with $T'\subseteq T.$
% 
%  
% 
% During such a call, a lot of 
%  (maximal) satisfiable subsets 
%  of T' will be computed.  (together with the corresponding model)
%  (*) Satisfiable subsets of T' can easily be extended to (not-neccesarily-maximal) satisfiable subsets of T (just check which of Ts constraints are satisfied in the accompanying model) 
%  
% THESE are the ones to be stored. 
%   OPTIIMIZATION 1: do not store all of them, only the subset-maximal one. If S1 and S2 are satisfiable subsets of T and $S1 \subseteq S2$, do not store S1. 
% 
%  
% 
% 
% Given a new OMUS call OMUS(T") with $T"\subseteq T$
% Take all SS of T.
% For each of them: take intersection with T". 
%     OPTIMIZATION 2: only keep the subset-maximal of those 
%     (this is an independent optimization from the previous one. It can be that after taking intersection, one of them is no longer subset-maximal!) 
% Use the SS as the start for the current call. During the current call more (M)SS will be generated. Always extend them to SS of T (see higher) and store (taking OPTIMIZATION 1 into account)     
% 
%  
% 
% 
% THat's it. 



