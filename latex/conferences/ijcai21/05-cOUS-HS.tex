The first two considerations from the previous section lead to the following definition. 

\begin{definition}
   Let $\formula$ be a formula, $f:2^{\formula} \to \nat$ a cost function and  $p$ a predicate $p: 2^{\formula}\to \{true,false\}$.  We call %a set $U\subseteq \formulag$ a \emph{$p$-constrained $f$-OUS} of \formulag ($(p,f)$-OUS) \tias{what with the OCUS name here?} \bart{I propsoe to say. We call 
    $\m{S} \subseteq \formula$ an OCUS of \formula (with respect to $f$ and $p$) if \begin{itemize}                                      
      \item $\m{S}$ is unsatisfiable,
      \item $p(\m{S})$ is true
      \item all other unsatisfiable $\m{S}'\subseteq \formula$ for which $p(\m{S}')$ is true satisfy $f(\m{S}')\geq f(\m{S})$.
    \end{itemize}
\end{definition}


If we assume that the predicate $p$ is specified itself as a CNF over (meta)-variables indicating inclusion of clauses of $\m{F}$, and $f$ is obtained by assigning a weight to each such meta-variable, then the complexity of the problem of finding an OCUS is the same as that of the SMUS (cardinality-minimal MUS) problem  \cite{ignatiev2015smallest}: the associated decision problem is $\Sigma^P_2$-complete. Hardness follows from the fact that SMUS is a special case of OCUS, containment follows - intuitively - from the fact that this can be encoded as an $\exists\forall$-QBF using a Boolean circuit encoding of the costs. 

Rephrased in terms of OCUS, the task of the procedure \onestep is to compute an OCUS of the formula $\formula := \formulac\land I\land \overline{\Iend\setminus I}$ with $p$ the predicate that holds for subsets  that contain exactly one literal of $\overline{\Iend\setminus I}$, see \cref{alg:oneStepOCUS}. 
%In the rest of this paper, we study (incremental) algorithms for computing an OCUS. 

%\emilio{why is the text formatting weird ? Due to the algorithm names ? }
In order to compute an OCUS of a given formula, we propose to build on the hitting set duality of \cref{prop:MCS-MUS-hittingset}. 
For this, we will assume to have access to a solver \cohs that can compute hitting sets of a given collection of sets that are \emph{optimal} (w.r.t.\ a given cost function $f$) among all hitting sets \emph{satisfying a condition $p$}. 
The choice of the underlying hitting set solver will thus determine which types of cost functions and constraints are possible. 
In our implementation we use a cost function $f$ as well as a condition $p$ that can easily be encoded as linear constraints, thus allowing the use of highly optimized mixed integer programming (MIP) solvers. The \cohs formulation is as follows:
%\bart{should be minimize or -f}\emilio{adapted}
\begin{align*}
\small
  minimize_S \quad & f(S) \\ 
  s.t. \quad & p(S) \\
       & sum(H) \geq 1, \quad &&\forall H \in \setstohit \\
       & s \in \{0,1\}, \quad &&\forall s \in S
\end{align*}
%\bart{This is not what the reviewers asked for! They asked for MIP models of our ``arbitrary objective functions''. A mip encoding of a generic hitting set problem with only some ``at least one'' constraints is not going to help us, I think. }
% well, its not arbitrary but linear, and it is a weighted sum; will have to do
%\bart{indeed, it is not arbitrary! But that's exactly the point: if we say ``we only support linear objective functions'', then that reviewers concern ``how will you encode this as MIP?'' is resolved. We simply did not say that in the prior work. In any case, the model also doesn't hurt and breaks text a bit... } 
where $S$ is a set of MIP decision variables, one for every clause in $\formula$. In our case, $p$ is expressed as $\sum_{s \in \overline{\Iend\setminus I}} s = 1$. 
%On top of that, the $p$ can be used to enforce that some constraints in \formula are hard constraints and should always be included in the hitting set. %make a distinction between hard constraints (those that \emph{must} be included in the OCUS), which can be useful in case constraints are reified using assumption literals, or to \emph{group} constraints that should be enabled/disabled simultaneously. 
%\emilio{not trivial at all!!! If I understand correctly C = Hard + indicator variables where p imposes constraints on not using the Hard, but only the indicator variables}
%\emilio{Added an example:}
%For example, when explaining logic grid puzzles, the cnf-translated hard constraints ($\mathcal{C}_{hard}$) are reified using assumption literals i.e weighted soft constraints ($\mathcal{C}_{soft}$). In that case, $\mathcal{C} = \mathcal{C}_{hard} \ \cup \ \mathcal{C}_{soft}$.
% When aiming to explain satisfaction problems in terms of the subset of constraints and literals needed to derive a new literal, the initial interpretation $I$ should consist of indicators literals for each (group of) constraint(s) as well as already known true literals. }
%\bart{Previous sentence instead of earlier ``indicator'' sentence?}
%
%In our application, 
$f$ is a weighted sum over the variables in $S$. For example, (unit) clauses representing previously derived facts can be given small weights and regular constraints can be given large weights, such that explanations are penalized for including constraints when previously derived facts can be used instead. %relevant facts are directly available.



\newcommand\onestepo{\ensuremath{\call{explain-One-Step-ocus}}\xspace}
\begin{algorithm}[t]
  \DontPrintSemicolon
  
  \caption{$\onestepo(\formulac,f,I,\Iend)$}
  \label{alg:oneStepOCUS}
  $p \leftarrow$ exactly one of $\overline{\Iend\setminus I}$\;
  \Return{$\comus(\formulac\land I\land \overline{\Iend\setminus I}, f, p)$} 
\end{algorithm}
\begin{algorithm}[t]
  \DontPrintSemicolon
  $\setstohit  \gets \emptyset$ \; %\label{omus-line1} 
  \While{true}{
    $\m{S} \gets \cohs(\setstohit,f,p) $  \;%\tcp*{\small Find \textb    $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \F''\}$ \;
% f{optimal} solution}
    % \tcp{\small set with all unique clauses from hitting set}
%     (sat?, $\kappa$) $\gets$ \texttt{SatSolver}($hs$)\;
    % \tcp{If SAT, $\kappa$ contains the satisfying truth assignment}
    % \tcp{IF UNSAT, $hs$ is the OUS }
    \If{ $\lnot \sat(\m{S})$}{\label{alg:ocus-sat-check}
      \Return{$\m{S}$} \;
    }
    $\m{S} \gets  \grow(\m{S},\F) $ \label{line:grow}\;
    $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \m{S}\}$ \;
  }
  \caption{$\comus(\formula,f,p)$ }
  \label{alg:comus}
\end{algorithm}
%\tias{H should be initialised in algo 3}

%\tias{I would not show the above one as it is rather vague \bart{I would disagree with the vagueness. It makes abstraction of several things (what is $p$? what is $f$? How is Grow and CondOptHS implemented? But in my opinion that is good, since it shows close relation to the basic OUS algo as well as illustrating what is really going on and modularity.}, but immediately rewrite it as Alg2 the singleStepExplain:}
%\bart{I would avoid that :-) That way we mix up ``how to compute constrained OUSs?'' with ``how to compute explanations using constrained OUSs?''. These are two different concerns. We should show that we also tackle the first .  That way, our new ``singlestepexplain2'' will also look a lot simpler than singleStepExplain1 (if we use one oracle call to cOUS}

%\begin{algorithm}[ht]
%  \caption{$\call{ExplainCSPcOUS}({\cal C},f)$}
%  \label{alg:explainCSPcOUS}
%$E \gets \langle \rangle$\;
%$I_{end} \gets optimalPropagate({\cal C})$\;
%$\formulag \gets {\cal C} \cup I_{end} \cup \overline{\Iend}$\;
%$\setstohit \gets \{\formulag \setminus \{{\cal C} \cup I_{end}\}\}$\;
%// preseeding\\
%\For{$l \in I_{end}:$}{
%  $\setstohit \gets \setstohit \cup \{\formulag \setminus \grow(-l,\formulag)\} $\;
%}
%$I \gets \emptyset$\;
%$p \gets \{$exactly one of $\overline{\Iend}$ in the hitting set$\}$\; %, none of $I_{end}$ can be in the hitting set$\}$\;
%
%\While{$I \neq I_{end}$}{
%	update $p$ such that none of $\{I_{end} \setminus I\}$ and none of $\bar{I}$ can be in the hitting set\;
%    $\m{S} \gets \comus(\formulag,f,p,\setstohit)$\;
%	$I_{\mathit{best}} \gets I\cap \m{S}$\;
%    ${\cal C}_{\mathit{best}}\gets{\cal C}\cap \m{S}$\;
%	$N_{\mathit{best}} \gets \{I_{end} \setminus I\} \cap optimalPropagate(\m{S}) $\;
%	add $\{I_{\mathit{best}} \wedge {\cal C}_{\mathit{best}} \implies N_{\mathit{best}}\}$ to $E$\;
%	$I \gets I \cup N_{\mathit{best}}$\; 
%  }
%\Return{E}\;
%\end{algorithm}
Our generic algorithm for computing OCUSs is depicted in \cref{alg:comus}. It combines the hitting set-based approach for MUSs of \cite{ignatiev2015smallest} with the use of a MIP solver for (weighted) hitting sets as proposed for maximum satisfiability \cite{DBLP:conf/sat/DaviesB13}. The key novelty is the ability to add structural constraints to the hitting set solver, without impacting the duality principles of \cref{prop:MCS-MUS-hittingset}, as we will show.

Ignoring \cref{line:grow} for a moment, 
the algorithm alternates calls to a hitting set solver with calls to a \sat oracle on a subset $\m{S}$ of $\formula$. 
In case the \sat oracle returns true, i.e., the subset $\m{S}$ is satisfiable, the complement of $\m{S}$ is a correction subset of $\m{F}$ and is added to \setstohit. %In this way, the hitting set size always grows   THIS IS PLAINLY UNTRUE. PLEASE DONT PUT THIS BAKC. and a hitting set $\m{S}$ will always be a subset of $\formula$.

Similar to OCUS the algorithm of \citet{ignatiev2015smallest}, our algorithm contains an (optional) call to \grow. 
% However, the grow is optional: ``\emph{one can avoid the computation of MCS $\m{C}$ and use any correction subset $\m{C}'$ s.t. $\m{C} \subseteq \m{C}'$... Therefore, the grow procedure can be skipped}''.
The purpose of the \grow is to expand a satisfiable subset of $\m{F}$ further, to find a smaller correction subset and as such find stronger constraints on the hitting sets. 
In our case, the calls for hitting sets will also take into account the cost ($f$), as well as the meta-level constraints ($p$); as such, it is not clear a priori which properties a good \grow function should have here.
% These extensions resulting in novel problems, and hence in this extended setting, we had to reconsider the question `what constitutes a good grow?'.
We discuss the different possible implementations of \grow later and evaluate their performance in \cref{sec:experiments}. For correctness of the algorithm, all we need to know is that it returns a satisfiable subset $\m{S}'$ of $\m{F}$ with $\m{S}\subseteq\m{S}'$.
%In our case, OCUS requires to account for the cost, as well as the meta-level constraints. One of the contributions of this work is experimentally validating that using cost-based grow heuristics significantly reduces the time to find an OCUS. To summarize, OCUS is based on implicit hitting set enu-meration, like the SMUS algorithm. The key differences with respect to SMUS are the use of a cost function and meta-
%constraints. These extensions resulting in novel problems and hence in this extended setting we had to reconsider the question “what constitutes a good grow?”
%
%Instead of directly adding the complement of $\m{S}$ to \setstohit as done by \citet{ignatiev2015smallest}, our algorithm includes a call to \grow, which extends $\m{S}$ into a larger subset of $\formula$ that is still satisfiable (if possible). 
%\bart{AGAIN: DOES IGNATIEV NOT HAVE A GROW? A reviewer also asked this. We anwsered the reviewer that he was right: ignatiev has a grow. 
%Emilio, please check the reviews, and definitely our response that no such things remain! We cannot claim that ignatiev does not grow, simply because they say it is optional. We discussed this many times already}
%%\tias{here base grow impls?} \bart{Your next sentence is enough for me here. Alternatively we can in one stentence say that we consider a greedy and a maxsat-based implementation? }
%We discuss the different possible implementations of \grow later and evaluate their performance in \cref{sec:experiments}.

Soundness and completeness of the proposal follow from the fact that all sets added to \setstohit are correction subsets, and \cref{thm:soundcomplete}, which states that what is returned is indeed a solution and that a solution will be found if it exists. 
 
 
 
\begin{theorem}\label{thm:soundcomplete}
  Let $\m{H}$ be a set of correction subsets of \formula. 
  If $\m{S}$ is a hitting set of \m{H} that is $f$-optimal among the hitting sets of \m{H} satisfying a predicate $p$, and  $\m{S}$ is unsatisfiable, then $\m{S}$ is an OCUS of \formula. 
  
  If  $\m{H}$ has no hitting sets satisfying $p$, then $\formula$ has no OCUSs.
\end{theorem}
\begin{proof}
For the first claim, it is clear that $\m{S}$ is unsatisfiable and satisfies $p$. Hence all we need to show is $f$-optimality of $\m{S}$.
  If there would exist some other unsatisfiable subset $\m{S}'$ that satisfies $p$ with $f(\m{S}')\leq f(\m{S})$, we know that $\m{S}'$ would hit every minimal correction set of \m{F}, and hence also every set in \m{H} (since every correction set is the superset of a minimal correction set).
  Since $\m{S}$ is $f$-optimal among hitting sets of $\m{H}$ satisfying $p$ and $\m{S}'$ also hits $\m{H}$ and satisfies $p$, it must thus be that $f(\m{S})=f(\m{S}')$. 
%  

The second claim immediately follows from \cref{prop:MCS-MUS-hittingset} and the fact that an OCUS is an unsatisfiable subset of $\formula$. 
\end{proof}
% 	We define $f = \sum w_i$ the weighted sum over the following clause weights $w_1 = 20, w_2=20, w_3=20, w_4=20, w_5=1, w_6=1, w_7=1$. with interpretation $\{x_1, \lnot x_2, x_3 \}$:
%\[ c_1 = \lnot x_1 \vee \lnot x_2 \vee x_3\] \[ c_2 = \lnot x_1 \vee  x_2 \vee x_3 \text{\hspace*{20pt}} c_3 = \lnot x_3 \vee \lnot x_2 \]
%	If we already know $I= \{x_1\}$ represented by $c_5$, then $\overline{\Iend\setminus I} = \{ x_2, \lnot x_3\}$ is defined by the clauses $c_6 \wedge c_7$:
%\[c_5 = x_1 \text{\hspace*{20pt}} c_6 = x_2 \text{\hspace*{20pt}} c_7 = \lnot x_3 \]
%We define $p\triangleq$ \textit{exactlyone($c_6$, $c_7$)} and $f = \sum w_i$ the weighted sum over the following clause weights $w_1 = 20, w_2=20, w_3=20, w_4=20, w_5=1, w_6=1, w_7=1$.
% 

Perhaps surprisingly, correctness of the proposed algorithm does \emph{not} depend on monotonicity properties of $f$ nor $p$. In principle, any (computable) cost function and condition on the unsatisfiable subsets can be used. In practice however, one is bound by limitations of the chosen hitting set solver.

As an illustration, we now provide an example of one call to $\onestepo$ (Algorithm 
\ref{alg:oneStepOCUS}) and the corresponding \comus-call (Algorithm \ref{alg:comus} in detail: 
\begin{example}
	Let $\formulac$ be a CNF formula over variables $x_1, x_2, x_3$ with the following four clauses:
		\[ c_1 := \lnot x_1 \vee \lnot x_2 \vee x_3 \qquad  c_2 := \lnot x_1 \vee  x_2 \vee x_3\] \[  c_3 := x_1 \qquad c_4 := \lnot x_3 \vee \lnot x_2 \]
	 If the current interpretation ($I$) is  $\{ x_1\}$, then the final interpretation ($\Iend$) is $\{x_1, \lnot x_2,  x_3\}$. In this case $\overline{\Iend\setminus I} =  \{x_2, \lnot x_3\}$. To define the input for the OCUS call, we add new clauses representing the already known facts and the to-be-derived facts: 
	 \[ c_5 = \{x_1\}\qquad  c_6=\{x_2\} \qquad  c_7 = \{\lnot x_3\}\]
	 We define $p\triangleq$ \textit{exactlyone$(c_6, c_7)$} and $f = \sum w_ic_i$ the weighted sum using clause weights $w_1 = 20, w_2=20, w_3=20, w_4=20, w_5=1, w_6=1, w_7=1$.
	 
	 The formula to be given to the is thus: 
% 	 Given \formulac, $f$, $\m{I}$ and $\Iend$ we have an OCUS-call with
	 \[\formula= \formulac\land I \land \overline{(\Iend\setminus I)} = \{ c_1 \wedge c_2 \wedge c_3\wedge c_4\wedge c_5\wedge c_6\wedge c_7\}\]	
	 
% 	Let $\setstohit$ be the collection of hitting sets. 
%	We can help the hitting set solver by bootstrapping it with $\setstohit  =\{ \{ c_5, c_6\} \}$, since we know at least one of the two clauses has to be taken.\bart{why would you do this if you ALSO add the exactly one constraint? This does not help you. Drop t his sentence?}
\setstohit is initialized as the empty set. At each iteration, the hitting set solver will search for a cost-minimal assignment that hits all sets in \setstohit and that furthermore contains exactly one of $c_5$ and $c_6$ (due to $p$).
%	We also add the `\textit{exactlyone}' constraint to the hitting set solver. \bart{The hitting set solver is provided p. You already defined p. Replace by: ``\setstohit is initialized as the empty set. At each iteration, the hitting set solver will search for a cost-minimal assignment that hits all sets in \setstohit and that furthermore contains exactly one of $c_5$ and $c_6$ (due to $p$)''.}
	Table \ref{tab:explanation-steps-expanded} sketches the steps required to find an OCUS for \formula given $f$ and $p$.\end{example}
\begin{table}[!h]
	\centering
	\begin{adjustbox}{max width=\columnwidth}
			\begin{tabular}{lcccc} %r||c|c|c|c}
				%\toprule
				&$\m{S}$ & $\sat(\m{S})$ & \grow($\m{S}$, $\formula$) & $\setstohit  \gets \setstohit  \cup \{  \formula \setminus \m{S}\}$\\ 
				\midrule
				0 & - &  -&-  & \rule{0pt}{2ex} $\emptyset$  \\
				%\midrule
				1 &$ \emptyset $ & $true$ & $\{c_1, c_2, c_4, c_6, c_7\}$    & $\{ \{c_5\}\}$   \\
				%\midrule	
				2 &$\{ c_5\}$  & $true$ & $\{ c_1, c_2, c_5, c_6\}$   & $\{ \{c_5\}, \{c_4, c_7\}\}$  \\  
%				& &  &    & $$\rule{0pt}{2ex} \\
				%\midrule
				3& $\{ c_5,c_7 \}$  & $true$ & $\{c_2, c_4, c_5, c_6,c_7 \}$  & $\{ \{c_5\},\{c_4, c_7\}, \{c_1\}\}$  \\  
				%\midrule
				\multirow{2}{*}{4}& \multirow{2}{*}{$\{ c_1, c_5,c_7 \}$}  & \multirow{2}{*}{$true$} & \multirow{2}{*}{$\{c_1, c_4, c_5,c_7 \}$}  & $\{ \{c_5\},\{c_4, c_7\}, \{c_1\}$  \\ 
				& &  &  & $\{c_2, c_6\}\}$  \\   
				%\midrule
				5&  $\{ c_1, c_2, c_5, c_7 \}$ & $false$ & & \\
				%\bottomrule
			\end{tabular}
	\end{adjustbox}
	\caption{Example of an OCUS-explanation computation.}
	\label{tab:explanation-steps-expanded}
\end{table}


%<<<<<<< HEA
The lines 1 to 4 represent the successive hitting sets $\m{S}$ computed by the hitting set solver. 
In this example, the \grow we used is the one called \emph{Max-Actual-Unif} in \cref{sec:experiments}. 
%The call to \grow exemplifies a call to \maxsat solver with hard clauses (or infinite weights) on the hitting set clauses and uniform weights on the remaining clauses of $\formula$.\bart{I don't think that this can at the moment be followed. I would say ``In this example, the \grow we used is the one called ''... in \cref{ref}.  } 
We further detail in section~\ref{sec:experiments} which \grow-configuration works best. 
%\egcomment{Since this was a remark of one of the reviewers I thought it would be nice to have an example of how an explanation is computed in practice}

%\egcomment{Something to add here @Bart ?}
%\emilio{Extending theorem \ref{thm:soundcomplete}, if $f$ is defined as a stricly increasing monotone function and p also satisfying the monotonicity property then \comus is guaranteed to be a \emph{minimal} unsatisfiable subset.
% \todo{Add proof ?}
% 
% \begin{proposition}
% The problem of deciding whetehr 
% 	\emilio{The complexity of extracting an \mm{\call{O(C)US}} is at least of the second level of polynomial hierarchy FP$^{\sum^{P}_2}$. }
% \end{proposition}
% %
% \begin{proof}
% 	\emilio{
% 	The complexity of extracting a cardinality-minimal MUS (SMUS) for an unsatisfiable CNF $\formula$ is of the second level of polynomial hierarchy FP$^{\sum^{P}_2}$ \cite{ignatiev2015smallest}. If uniform weights are applied to \comus then it is reduced to extracting an SMUS.}
% 	%the \omus extraction is reducible to an SMUS-extraction on a relaxed formula $\formula^{R}$ in polynomial time. 
% %
% %		\noindent\textit{Reduction.} Associate for each clause $c_i \in \formula$ of weight $w_i=n \in \mathbb{N}^+$, the relaxation variables $r_{i,j}$ where $j \in 1..n-1$:
% %		\[ \formula^{R}  \triangleq \bigcup_{c_i \in \formula}(c_i \leftrightarrow (r_{i, 1} \wedge ..  r_{i, n-1})) \]
% %	$F^{R} \triangleq \cup \$
% %	, by adding $w_i - 1$ relaxation variables $(r_{i,1} \wedge .. \wedge r_{i,n-1}) \leftrightarrow c_i$, the problem of OUS is reduced to the problem of SMUS.
% \end{proof}
% 




% Now, since the search for optimal hitting sets is --- in implicit hitting set algorithms --- usually done with a MIP solver, it suffices to express the predicate $p$ as constraints on the MIP. Since the variables of the MIP encoding represent inclusion of certain constraints in the unsatisfiable subset, this is simple for  the 3 constraints that we need to obtain meaningful explanations. %needed to have meaningful in practice only predicates $p$ that can easily be encoded in MIP are useful. In such cases, we can directly use the MIP solver to implement \cohs as well. 

% \paragraph{Application to Explanations}
% %To apply this idea to the context of explanations, we note that at each step, the current interpretation, will be fixed. 
% %At each step, we are looking for an OUS that contains \emph{exactly one} negation of a derivable literal. 
% %Such an exactly-one constraint is easily expressible in MIP.
% %Furthermore, also the ``subtheory constraint'', as introduced for incremental MUS solving can be expressed in MIP. Namely, in \cref{sec:incremental}, we assumed that each OUS call would be done given a subtheory of the original theory. However, constraints of the form ``the OUS should be a subset of the given set \formula'' are easily expressible in MIP as well. 
% %As such, the idea of constrained OUS computation is actually more general than the formalization of incremental OUS. 
% % 
% Given such a constrained OUS algorithm, the procedure to find the single best explanation step now simplifies to \cref{alg:singleStepExplain3}.
% 
% \begin{algorithm}[t]
%   \caption{$\call{bestStep--c-OUS}({\cal C},f,I,I_{end})$}
%   \label{alg:singleStepExplain3}
% $\formulag \gets {\cal C} \cup I_{end} \cup \overline{\Iend}$\;
% set $p$ such that exactly one of $\overline{\Iend}$ in the hitting set \textit{and} none of $\{I_{end} \setminus I\}$ \textit{and} none of $\bar{I}$ can be in the hitting set\;
% \Return{$\comus(\formulag,f,p)$}\;
% \end{algorithm}

% \tias{hard/soft temporarily hidden}
% \ignore{

