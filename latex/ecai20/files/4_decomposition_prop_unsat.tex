In this section, we tackle the goal of searching for a sequence of $E_i \wedge S_i \rightarrow N_i$ explanations that is as simple to understand as possible, and that ends in the the most complete interpretation $I_n$ achievable with the constraints, e.g. a total interpretation.

Ideally, we would generate all explanations of each fact in $I_n$, and search for the lowest scoring combination among those. However, the number of explanations for each fact quickly explodes with the number of constraints, and is hence not feasibly to compute. Instead, we will iteratively construct the sequence, by generating candidates for a given partial interpretation and searching for the smallest one among those.

\paragraph{Sequence construction}
We aim to minimize the total cost of the explanations of the sequence, measured with an aggregate over explanation costs $f(E_i, S_i, N_i)$ for some aggregate like $max()$ or $average()$. Instead of trying to globally optimize it, we will greedily and incrementally build the sequence, each time searching for the cheapest next explanation given the current partial interpretation. 

Note that this greedy approach may not find the optimal sequence \tias{Bart, did you say that it does for max? Do we know for sure that there might not be a case where 2,4 are costs, while 3,3 are also costs and greedy will choose the first?}. Also note that as by definition $I_n$ is the intersection of all valid solutions of \allconstraints, there are no \textit{choices} to be made to reach $I_n$ and all reasoning steps that derive a fact of $I_n$ inherently do not lead to failure and can hence be greedily added. \tias{New realization: in the 'intersection' setting, we must filter such that $N_i = N_i \cap I_n$ with $I_n$ computed at the beginning in order not to greedily add something 'else' that leads to failure (which it can not see yet for a small $C$)}

Algorithm \ref{alg:main} formalizes the greedy construction of the sequence. We assume that the \textit{propagate} function is optimal in that it computes the intersection of all valid solutions, but our approach is also valid for propagation with other levels of consistency.

\begin{algorithm}
%  \begin{algorithmic}
$C \gets \allconstraints$\;
$I_n \gets$ propagate$(C)$\;
Seq $\gets$ empty sequence\;
$I \gets \{\}$\;
\While{$I \neq I_n$}{
  $(E, S, N) \gets $min-explanation$(I, C, I_n)$; \textit{\footnotesize // has $N \subseteq I_n$} \\
  append $(E, S, N)$ to Seq\;
  $I \gets I \cup N$
}
% \end{algorithmic}
\caption{High-level greedy sequence-generating algorithm.}
\label{alg:main}
\end{algorithm}

\paragraph{Candidate generation}
The main challenge is finding the best scoring explanation, among all reasoning steps that can be applied for a given partial interpretation $I$. We first look at how to enumerate a set of candidate explanations given a set of constraints.

For a set of constraints $C$, we can first use propagation to get the set of new facts that can be derived from a given partial interpretation $I$ and the constraints $C$. For each new fact $a$ that is also in $I_n$, we wish to find a minimal explanation $(E \subseteq I, S \subseteq C)$ that explains $a$. We hence first find all new facts, and then refine for each fact what a minimal explanation is.

The code below shows our proposed algorithm. The key part of the algorithm is on line \ref{line:mus} where we find an explanation of a single new fact $a$ by searching for a \textit{minimal unsatisfiable core} that includes $\neg a$. We do this over the literals of $I$ as well as over newly introduced \textit{reified} literals for every constraint in $C$. The result will be a minimal unsatisfiable core $\neg a \wedge E \wedge S$ where $E \subseteq I$ and $S \subseteq C$. The latter can in fact be extracted by observing which of the reified literals are set to \textit{true} in the MUS. From $UNSAT(\neg a \wedge E \wedge S)$ we know that $SAT(E \wedge S \rightarrow a)$.

We search for minimal unsat cores to avoid redundancy in the explanations. To avoid redundancy at the sequence level, we wish to avoid generating multiple $E \wedge S \rightarrow a, E \wedge S \rightarrow b$ explanations with the same $(E, S)$. Hence, we choose to generate candidate explanations at once for all implicants of $(E, S)$ on line~\ref{line:implicants}. Note that the other implicants $A \setminus \{a\}$ may have simpler explanations that may be found later in the for loop, hence we do not remove them from $J$.

% % \comment{
\begin{algorithm}
% 
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwComment{command}{/*}{*/}

 \Input{A partial interpretation $I$ and a set of constraints $C$}
% \end{algorithm}
% 
% \Require{$I = $ partial interpretation, $C =$ a set of constraints}
% \Fn(\tcc*[h]{algorithm as a recursive function}){BLA{some args}}{

% \Function{candidate-explanations}{I, C}
  Candidates $\gets \{\}$\;
  $J \gets$ propagate$(I \wedge C)$\;
  \For{$a \in J \setminus I$}{ 
  \tcp{Minimal expl. of each new fact}
    $X \gets MUS(\neg a \wedge I \wedge reify(C))$ \label{line:mus}\;
    $I' \gets I \cap X$\;
    $C' \gets C \cap X$\;
    $A \gets$ propagate$(I' \wedge C')$; \textit{\small // all implied facts}\label{line:implicants}\\
    add $(I', C', A)$ to Candidates
  }
  \Return{Candidates}
% \EndFunction
\caption{candidate-explanations$(I,C)$}

\label{alg:cand}
\end{algorithm}

% }

We assume the use of a standard MUS algorithm, e.g. that searches for a satisfying solution and if a failure is encountered, the resulting Unsat Core is shrunk to a Minimal Unsat Core~\cite{}. While computing a MUS in this way may be computationally demanding, it is far less demanding than enumerating all MUS's (of arbitrary size) as candidates. 
Hence, the result of the MUS call on line~\ref{line:mus} is \textit{an} unsatisfiable core that is \textit{subset-minimal}, but not \textit{size-minimal}. That is, the unsat core can not be reduced further, but there could be another minimal unsat core whose size is smaller.

%If we want to find the best ordering (TODO), we need the absolute minimal MUS, which is typically only a few constraints.
%This relates to the objective function...

To avoid having te search for all MUS's for each new fact, we use the observation that typically a small (1 to a few) number of constraints is sufficient to explain the reasoning. A small number of constraints is also preferred in terms of easy to understand explanations. We will hence not call \textit{candidate-explanations} with the full set of constraints \allconstraints, but we will iteratively grow the number of constraints used. 
%\bart{ In practice this means that we are doing some form of prioritized explanatoin. 
%First priority: keep the number of constraints small. Second priority, size (which also takes structure into acccount}

\paragraph{Smallest explanations and cost functions}
\bart{Do we have examples where $A$ is to be taken into account for the size! } 
In the following, we assume that we have a cost function $f(I', C', A)$ that returns a score for every possible explanation. The goal will be to find the lowest scoring explanation. We make one further assumption, namely that we have an optimistic estimate $g(C')$ computed on only the constraint part of the explanation, and that $\forall I', A, g(C') \leq f(I', C', A)$. This is for example the case if $f$ is an additive function, such as $f(I', C', A) = f_1(I') + f_2(C') + f_3(A)$ where $g(C') = f_2(C')$ assuming $f_1$ and $f_3$ are always positive.

We can then search for the smallest explanation among the candidates found, by searching among increasingly worse scoring $C'$ as shown in the code below, which makes use of the candidate-explanations function defined above.

% \bart{TODO PUT ALGO BACK}
\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwComment{command}{/*}{*/}
\SetKw{Break}{break}


 \Input{A partial interpretation $I$ and a set of constraints $C$}
  Candidates $\gets \{\}$\;
  $J \gets$ optimal-propagate$(I \wedge C)$\;
  \For{$C' \subseteq C$ ordered by $g(C')$}{ 
    \If{$g(C') < min(f($Candidates$))$}{
        \Break\;}
     cand $\gets$ candidate-explanations$(I, C')$\; 
     add to Candidates all cand$_i$ with corresp. value $f($cand$_i)$\;
     }  
          \Return{smallest scoring candidate of Candidates}
 % \EndFunction
\caption{smallest-explanations$(I,C)$}
\label{alg:cand}
\end{algorithm}


\tias{Moved the below paragraph here, should be part of the cost-optimal imho}
In principle our problem of finding subset-minimal (or even, see the next paragarph, cost-optimal) justifications could be solved by specifying it in such a way in second-order logic and subsequently finding models for it, e.g. by a reduction to QBF \cite{kr/BogaertsTS16,kr/vanderHallenJ18}. 
However, since this is already an $\exists\forall\exists SO$ specification, we expect this approach not be feasible (this expectation remains to be verified in future work). 


\textbf{That is it for now... more optimisations are possible, basically in the way we search over the C', like, cache Candidates so that its minimum value is maintained (and do an 'update' to it before starting to potentially clean it); do not search for $C'' \supset C'$ where C' has some explanations; perhaps we can find out that at some point we no longer need to optimalprop some $C'$? (e.g. a 'fully used' clue?)}

\paragraph{Rest is braindump part}
We assume every constraint $c \in C$ has a weight $w(c)$, and we assume every literal has a weight when used as previously derived fact $w_l(f)$, and when it is a newly derived fact $w_r(f)$. The total cost of an explanation is then:
$$ cost(I', C', A) = w_1*(\sum_{i \in I'} w(i)) + w_2*(\sum_{c \in C'} w(c)) + w_3*(\sum_{a \in A} w(a))$$

The $w_1, w_2, w_3$ can be used to trade-off weights of constraints and facts globally, e.g. to mimic a lexicographic ordering.

In this work, we assume that the weights are manually set based on domain knowledge. For the logic grid puzzles, we prefer small numbers of constraints primarily, so $w_2$ has a high value ($w_2=100$). Among constraints, clues get a weight of $1$ + $0.01$ times their index in the list, such that clues higher in the list get preference. Implicit constraints are even more preferred with a weight of $0.5$ for transitivity constraints and $-1$ for bijection constraints, as users typically complete the latter immediately.
Previously used facts are uniformily weighted $w_1=1$ and $w(i)=1, \forall i$. The number of newly derived facts matter less, hence $w_3=0.1$ though with a slight preference for positive literals which get a weight of $w(a)=0.1$ when $a$ is positive, else $w(a)=1$.

Based on this cost function, it is clear that small $C'$ sets should be preferred, but the MUS search does not ensure to find the smallest possible MUS, just that the MUS is minimal. Hence, we will do a 'level-wise' search for increasing constraint set cost $w_2*(\sum_{c \in C'} w(c))$.

\textbf{Should we do something with checking whether a (set of) constraints is implied, so that we never have to check it again?}


%
%
%	
%
%\begin{verbatim}
%C /\ B -> A
%
%given(I facts, C cons, D impl cons):
%
%store = {}
%for c in Cons:
%  I' <- optimal_propagate(I, c /\ D) # is this needed, with D added?
%  let A = I \setminus I'
%  if |A| != 0:
%    # refine 'c'... because?
%    forall a in A:
%      X = MUS(not a /\ I /\ c /\ D)
%      if c \notin X:
%        # some special case...? should be avoided
%      else:
%        let B' is I \intersect X
%        let D' is D \intersect X
%        # find other implications of D' /\ c
%        A' = optimal_propagate(B', c /\ D')
%        add to store (c /\ D', B', A') as candidate explanation
%
%if |store| == 0:
%  # TODO: more constraints at same time.
%  # perhaps binary search makes sense?
%  cut C in C1 and C2
%  I'1 <- optimal_propagate(I, C1 /\ D)
%  if not empty, recursively refine
%  if recursive children were empty:
%    no... can not recursively split, may be combination across split
%\end{verbatim}

