% \todo{make naming consistent: OCUS instead of OCUS}
%<<<<<<< HEAD
%Our OCUS algorithm in~\ref{alg:comus} is generic and can also be used to find OUSs, i.e by setting $p$ simply to \textit{true}. % In algorithm \ref{alg:comus}, this boils down to replacing line 2 with $\cohs(\setstohit,f,True)$
%=======
Algorithm~\ref{alg:comus} is generic and can also be used to find (unconstrained) OUSs. namely with a trivially true $p$. % In algorithm \ref{alg:comus}, this boils down to replacing line 2 with $\cohs(\setstohit,f,True)$
%>>>>>>> 9bb2659817f6b9551a7557006e9a509a509d6f19
However, its constrainedness property allows to removing the need to compute a MUS/OUS for every literal, and hence decreasing the complexity of explanation sequence generation from $O(n^2)$ calls to MUS/OUS to $O(n)$, namely, once for every step in the sequence.

We now discuss optimizations to the OCUS algorithm that are specific to explanation sequence generation, though they can also be used when other forms of domain knowledge are present. 
%In this section, we discuss optimizations to the basic algorithm to compute OCUSs. 
%While our optimizations are inspired by the explanation generation problem, and tailored to solving this as efficiently as possible, the ideas presented here can also 
 
\paragraph{Incremental OCUS computation}
%While searching for unsatisfiable subsets satisfying structural constraints already greatly reduces the number of calls to the unsatisfiable subset oracle, i
Inherently, generating a sequence of explanations still requires as many OCUS calls as there are literals to explain. 
Indeed, a greedy sequence construction algorithm %high-level algorithm, not explicitly portrayed in the current paper, 
calls \onestepo iteratively with a growing interpretation $I$ until $I=\Iend$.

All of these calls to \onestepo, and hence OCUS, are done with very similar input (the set of constraints does not change, and the $I$ slowly grows between two calls). For this reason, it makes sense that information computed during one of the earlier stages can be useful in later stages as well. 

The main question is, suppose two \comus calls are done, first with inputs $\formula_1$, $f_1$, and $p_1$, and later with $\formula_2$, $f_2$, and $p_2$; how can we make use as much as possible of the data computations of the first call to speed-up the second call? The answer is surprisingly elegant. The most important data \comus keeps track of  is the collection \setstohit of sets-to-hit.

This collection in itself is not useful for transfer between two calls, since -- unless we assume that $\formula_2$ is a subset of $\formula_1$, there is no reason to assume that a set in $\setstohit_1$ should also be hit in the second call. 
However, each set $H$ in $\setstohit$ is the complement (with respect to the formula at hand) of a \emph{satisfiable subset} of constraints, and this satisfiability remains true. 
Thus, instead of storing $\setstohit$, we can keep track of a set \satsets of \emph{satisfiable subsets} (the sets $\m{S}$ in the \comus algorithm). 
When a second call to \comus is performed, we can then initialize $\setstohit$ as the complement of each of these satisfiable subsets with respect to $\formula_2$, i.e., \[\setstohit\gets \{\formula_2\setminus \m{S}\mid \m{S}\in \satsets\}.\]

The effect of this is that we \textit{bootstrap} the hitting set solver with an initial set $\setstohit$. For hitting set solvers that natively implement incrementality, we can generalize this idea further: we know that all calls to $\comus(\formula,f,p)$ will be cast with $\formula \subseteq \m{C}\cup \Iend \cup \overline{\Iend \setminus I_0}$, where $I_0$ is the start interpretation. Since our implementation uses a MIP solver for computing hitting sets (see Section~\ref{sec:backgr}), and we have this upper bound on the set of formulas to be used, we can initialise all relevant decision variables once. To compute the conditional hitting set for a specific $\formulac\cup I\cup \overline{\Iend\setminus I} \subseteq \m{C}\cup \Iend \cup \overline{\Iend \setminus I_0}$ we need to ensure that the MIP solver only uses literals in $\formulac\cup I\cup \overline{\Iend\setminus I}$, for example by giving all other literals infinite weight in the cost function. In this way, the MIP solver will automatically maintain and reuse previously found sets-to-hit in each of its computations. 
% \emilio{need to re-read reviews, reviewers asked (?) for simple MIP model of the problem}

% \paragraph{Using Initialization to Add Domain Knowledge} ? 
% \todo{TO BE DETERMIEND IF WE WANT TO SAY SOMETHING AOBUT PRESEEDING.}\bart{ I still dont believe it makes a difference. TO be tested first!} {\color{red}The current experiments confirm (finally) that this is useless. So let us just drop it.  Honestly, this gives me more trust in the experiments :-)   }



\paragraph{Domain-Specific Implementations of \grow} \label{para:domainspecificgrow}
%In the context of explanations we are in, we have a way of generating large satisfiable subsets quite easily and hence we conjecture that implementing a domain-specific \grow will be more efficient than generic version of the \grow algorithm. 

The goal of the \grow procedure is to turn $\m{S}$ into a larger subformula of $\formula$. The effect of this is that the complement added to \setstohit will be smaller, and hence, a stronger restriction for the hitting set solver is found.  

Choosing an effective \grow procedure requires finding a difficult balance: on the one hand, we want our subformula to be as large as possible (which ultimately would correspond to computing the maximum satisfiable subformula), %to derive the most precise information we can get, 
but on the other hand we also want the procedure to be very efficient as it is called in every iteration. 
%<<<<<<< HEAD
%%\deleted{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$, however, by doing that we expect the \grow costs to be too high to make the overall OCUS algorithm faster.}
%Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$. However, as will become clear from the experiments, the cost of such a \grow is too high to make the overall OCUS algorithm faster. 
%=======
%\deleted{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$, however, by doing that we expect the \grow costs to be too high to make the overall OCUS algorithm faster.}
%\bart{This is extremely confusing. You seem to suggest here that we do NOT want to do maxsat. But we do!Ã¨ I would not include this here. Do we have evidence that a comlete maxsat would be better (if we ignore the cost of the grow? I don't know if it is} 
%\emilio{Indeed, if the computation cost of the \grow were not important, we could use a \maxsat call to obtain a subset-maximal subset of $\formula$. However, as will become clear from the experiments, the cost of such a \grow is too high to make the overall OCUS algorithm faster. }
%>>>>>>> 9bb2659817f6b9551a7557006e9a509a509d6f19

For the case of explanations we are in, we make the following observations: 
\begin{itemize}
%<<<<<<< HEAD
% \item Our formula at hand (using the notation from the \onestepo algorithm) consists of three types of clauses: 1) (translations of) the problem constraints (this is \formulac) 2) literals representing the assignment found thus far (this is $I$) and 3) the negations of literals not-yet-derived (this is $\overline{\Iend\setminus I}$). 
%=======
 \item Our formula at hand (using the notation from the \onestepo algorithm) consists of three types of clauses: \begin{inparaenum}\item  (translations of) the problem constraints (this is \formulac) \item literals representing the assignment found thus far (this is $I$), and \item the negations of literals not-yet-derived (this is $\overline{\Iend\setminus I}$). \end{inparaenum}
%>>>>>>> 9bb2659817f6b9551a7557006e9a509a509d6f19
 \item $\formulac$ and $I$ together are satisfiable, with assignment $I_{end}$, and \emph{mutually supportive}, by this we mean that making more constraints in \formulac true, more literals in $I$ will automatically become true and vice versa. 
 \item The constraint $p$ enforces that each hitting set will contain \textbf{exactly} one literal of  $\overline{\Iend\setminus I}$
\end{itemize}
Since the restriction on the third type of elements of $\formula$ are already strong, it makes sense to use the \grow(\m{S},\F) procedure to search for a \emph{maximal} satisfiable subset of $\formulac\cup I$ with hard constraints that $\m{S}$ should be satisfied, using a call to an efficient  (partial) \maxsat solver. Furthermore, we can initialize this call as well as any call to a \sat solver with the polarities for all variables set to the value they take in $\Iend$. %This is likely to lead to a fairly large satisfiable subset and hence a good correction set. 

We evaluate different grow strategies in the experiments section, including the use of partial \maxsat as well as weighted partial \maxsat based on the weights in the cost function $f$.

\paragraph{example 1 (cont.)} Considering line 2 of \ref{tab:explanation-steps-expanded}, \emph{Max-Actual-Unif}, when the \maxsat solver has to branch, it will favor assigning variables of the full interpretation.

% \todo{Bart from here}


% \todo{``old maxsat'' comes here}



